# (PART) Penalized Regressions {-}

# Parametric models in prediction {-}

Remember, in simple regression or classification problems, we cannot train a parametric model in a way that the fitted model minimizes out-of-sample prediction error.  We could and did the fit the models **manually** by adding or removing predictors and their interactions and polynomials.    Penalization in regressions (also called as regularization or shrinkage) is an alternative and automated fitting procedure, which can yield better prediction accuracy and model interpretability by preventing overfitting.

Two methods, Ridge and Lasso, are two of well-known benchmark techniques that reduce the model complexity and prevent overfitting resulting from simple linear regression. (See [LASSO AND THE METHODS OF CAUSALITY](https://skranz.github.io)) [@Kranz_2021]

As we have seen in earlier chapters, by dropping a variable in a regression it is possible to reduce the variance at the cost of a negligible increase in bias. In fitting the predictive model, some of the variables used in a multiple regression model may not be well associated with the response. Keeping those "irrelevant" variables often leads to unnecessary complexity in the resulting model. Sometimes this process, - removing irrelevant variables - is called as "feature selection", which sets the corresponding coefficients to zero.

In addition to variable selection, we can also constrain or shrink the estimated coefficients. If the number of observations is not much larger than the number of variables, this regularization can lead to substantial improvements in the out-of-sample prediction accuracy.  
As we have seen before, one can fit the following standard linear model, which describes the relationship between $Y$ and a set of $X$ variables, by using least squares.  

$$
Y=\beta_{0}+\beta_{1} X_{1}+\cdots+\beta_{p} X_{p}+\epsilon
$$

Although this model is linear, one can obtain a non-linear nature of the relationship between $Y$ and $X$'s by adding polynomials and interaction that increases the the dimension of the model beyond what the dataset provides.  There are several approaches involving fitting a model that incorporates features selection.  Here, we will see only the "shrinkage" method that shrinks the estimated coefficients towards zero.  The two best-known techniques for shrinking the regression coefficients towards zero are ridge regression and the lasso.  

# Ridge

We know that the least squares fitting procedure is that one estimates $\beta_{0}, \beta_{1}, \ldots, \beta_{p}$ that minimize the residual sum of squares:

$$
\mathrm{RSS}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}
$$
Ridge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity.

$$
\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p} \beta_{j}^{2} =\mathrm{RSS}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}
$$

Where $\lambda$ is the hyperparameter that can be tuned by cross-validation and grid search.  The additional terms is a constraint, $\lambda \sum_{j} \beta_{j}^{2}$, which is also called shrinkage penalty (hence, the title of the lecture: `Penalized Regressions`).  This type of penalty is called as $\ell_{2}$.  As with OLS, this cost function tries to minimize RSS but also penalizes the size of the coefficients.

The hyperparameter parameter $\lambda$ controls the relative impact of the penalization on the regression coefficient estimates. When $\lambda = 0$, the cost function becomes RSS, that is the cost function of OLS and the estimations produce the least squares estimates. However, as $\lambda$ gets higher, the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero. Note that, the shrinkage penalty is applied to slope coefficients not to the intercept, which is simply a measure of the mean value of the response, when all features are zero.

Lets apply this to the same data we used earlier, `Hitters` from the [ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/) [@ISLR_2021] package:

```{r, warning=FALSE, message=FALSE}
library(ISLR)

remove(list = ls())

data(Hitters)
df <- Hitters[complete.cases(Hitters$Salary), ]
```

We will use `glmnet` to fit a ridge regression.  This function has slightly different syntax from other model-fitting functions that we have used so far in this book, such as the `y ~ x` syntax.  Therefore before we execute the syntax, we have the prepare the model so that `x` will be a matrix and `y` will be vector.  Hence `X` matrix has to be prepared before we proceed.  The other important point is that `NA` are not allowed in `glmnet`.

```{r, warning=FALSE, message=FALSE}
X  <- model.matrix(Salary~., df)[,-1]
y <- df$Salary

# look at the difference b/w X and df
head(X)
str(df)
```

The `glmnet` package is maintained by Trevor Hastie who provides a friendly [vignette](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html) [@Hastie_glmnet].  They describe the importance of `model.matrix()` in `glmnet` as follows:

> (...)particularly useful for creating $x$; not only does it produce a matrix corresponding to the 19 predictors but it also automatically transforms any qualitative variables into dummy variables. The latter property is important because `glmnet()` can only take numerical, quantitative inputs.
>

The `glmnet()` function has an `alpha` argument that determines what type of model is fit. If `alpha = 0` then a ridge regression model is fit, and if `alpha = 1` then a lasso model is fit.  Here is the example for a ridge regression:

```{r, warning=FALSE, message=FALSE}
library(glmnet)
grid = 10^seq(10, -2, length = 100)
model <- glmnet(X, y, alpha = 0, lambda = grid)
```

As you can see, we didn't do an explicit grid search by cross validation.  By default, the `glmnet()` function performs ridge regression for an automatically selected range of $\lambda$ values.  It ranges from the null model - only intercept when $\lambda$ is at the upper bound and the least squares fit when the $\lambda$ is at lower bound.  The application above is to show that we can also choose to implement the function over a grid of values.  Moreover, the `glmnet()` function standardizes the variables so that they are on the same scale. To turn off this default setting, use the argument `standardize=FALSE`.  

The methods here, ridge and lasso, are parametric models.  Unlike non-parametric methods, each model is defined by a set of parameters or, as in our case, coefficients.  Therefore, when we do a grid search, each value of the hyperparameter is associated with one model defined by a set of coefficients.  In order to see the coefficients we need to apply another function, `coef()`.  Remember, we have 100 hyperparameters.  Hence, `coef()` produces a 20 x 100 matrix, with 20 rows (one for each predictor, plus an intercept) and 100 columns (one for each value of $\lambda$).  

```{r, warning=FALSE, message=FALSE}
dim(coef(model))
coef(model)[1:10, 1:10]
```

Due to the penalty, we expect the coefficient estimates to be much smaller, when a large value of $\lambda$ is used.  For example:

```{r, warning=FALSE, message=FALSE}
model$lambda[50]
coef(model)[, 50]
```

And,  

```{r, warning=FALSE, message=FALSE}
model$lambda[60]
coef(model)[, 60]
```

We will generally use the `predict()` function as before.  But, here we can also use it to estimate the ridge regression coefficients for a new value of $\lambda$.  Hence, if we don't want to rely on the internal grid search provided by `glmnet()`, we can do our own grid search by `predict()`.  This is an example when $\lambda = 50$, which wasn't in the grid.    

```{r, warning=FALSE, message=FALSE}
predict(model, s = 50, type = "coefficients")
```

There are two ways that we can train ridge (and Lasso): (1) to use our own training algorithm; (2) rely on `'glmnet` internal cross-validation process. Here is an example for our own training algorithm for training ridge regression:   

```{r, warning=FALSE, message=FALSE, cache=TRUE}
grid = 10^seq(10, -2, length = 100)

MSPE <- c()
MMSPE <- c()

for(i in 1:length(grid)){
  for(j in 1:100){
    set.seed(j)
    ind <- sample(nrow(df), nrow(df), replace = TRUE)
    ind <- unique(ind)
    train <- df[ind, ]
    xtrain <- model.matrix(Salary~., train)[,-1]
    ytrain <- df[ind, 19]
    test <- df[-ind, ]
    xtest <- model.matrix(Salary~., test)[,-1]
    ytest <- df[-ind, 19]
  
    model <- glmnet(xtrain, ytrain, alpha = 0, lambda = grid[i], thresh = 1e-12)
    yhat <- predict(model, s = grid[i], newx = xtest)
    MSPE[j] <- mean((yhat - ytest)^2)
    }
  MMSPE[i] <- mean(MSPE)
}

min(MMSPE)
grid[which.min(MMSPE)]
plot(log(grid), MMSPE, type="o", col = "red", lwd = 3)
```

What is the tuned model using the last training test with this $\lambda$?  

```{r, warning=FALSE, message=FALSE}
lam <- grid[which.min(MMSPE)]
coeff <- predict(model, s = lam , type = "coefficients", newx = xtrain)
coeff
```

We may want to compare the ridge with a simple OLS:

```{r, warning=FALSE, message=FALSE, cache=TRUE}
MSPE <- c()

for(j in 1:100){
  set.seed(j)
  ind <- sample(nrow(df), nrow(df), replace = TRUE)
  ind <- unique(ind)
  train <- df[ind, ]
  test <- df[-ind,]
  
  model <- lm(Salary~., data = train)
  yhat <- predict(model, newdata = test)
  MSPE[j] <- mean((yhat - test$Salary)^2)
}
mean(MSPE)
plot(MSPE, col = "blue", lwd = 2)
summary(model)
```
  
The second way is the `glmnet` internal training process, `cv.glmnet`, which is the main function to do cross-validation along with various supporting methods such as plotting and prediction.  A part of the following scripts follows the same algorithm as the one in the book (ISLR p.254).  This approach uses a specific grid on $\lambda$.    

```{r, warning=FALSE, message=FALSE, cache=TRUE}
X  <- model.matrix(Salary~., df)[,-1]
y <- df$Salary

# With a defined grid on lambda
bestlam <- c()
mse <- c()
grid = 10^seq(10, -2, length = 100)

for(i in 1:100){
  set.seed(i)
  train <- sample(1:nrow(X), nrow(X)/2) # arbitrary split
  test <- c(-train)
  ytest <- y[test]

  #finding lambda
  cv.out <- cv.glmnet(X[train,], y[train], alpha = 0)
  bestlam[i] <- cv.out$lambda.min

  #Predicting with that lambda
  ridge.mod <- glmnet(X[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
  yhat <- predict(ridge.mod, s = bestlam, newx = X[test,])
  mse[i] <- mean((yhat - ytest)^2)
}
mean(bestlam)
mean(mse)
plot(bestlam, col = "blue")
plot(mse, col = "pink")
```

Now the same application without a specific grid:  

```{r, warning=FALSE, message=FALSE, cache=TRUE}
X  <- model.matrix(Salary~., df)[,-1]
y <- df$Salary

bestlam <- c()
mse <- c()

# Without a defined grid on lambda
for(i in 1:100){
  set.seed(i)
  train <- sample(1:nrow(X), nrow(X)/2) # arbitrary split
  test <- c(-train)
  ytest <- y[test]
  
  cv.out <- cv.glmnet(X[train,], y[train], alpha = 0)
  yhat <- predict(cv.out, s = "lambda.min", newx = X[test,])
  mse[i] <- mean((yhat - ytest)^2)
}

mean(mse)
plot(mse, col = "pink")
```

Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As $\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. Ridge regression works best in situations where the least squares estimates have high variance.
