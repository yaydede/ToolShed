<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 35 Regularized covariance matrix | Toolbox for Social Scientists and Policy Analysts</title>
  <meta name="description" content="Chapter 35 Regularized covariance matrix | Toolbox for Social Scientists and Policy Analysts" />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 35 Regularized covariance matrix | Toolbox for Social Scientists and Policy Analysts" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://yaydede.github.io/ToolShed//png/cover2.png" />
  
  <meta name="github-repo" content="yaydede/ToolShed_draft" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 35 Regularized covariance matrix | Toolbox for Social Scientists and Policy Analysts" />
  
  
  <meta name="twitter:image" content="https://yaydede.github.io/ToolShed//png/cover2.png" />

<meta name="author" content="Yigit Aydede" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="fundementals.html"/>
<link rel="next" href="r-lab-1---basics-i.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.24/datatables.js"></script>
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Toolbox</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who"><i class="fa fa-check"></i>Who</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> How we define Machine Learning</a></li>
<li class="chapter" data-level="2" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>2</b> Preliminaries</a>
<ul>
<li class="chapter" data-level="2.1" data-path="preliminaries.html"><a href="preliminaries.html#data-and-dataset-types"><i class="fa fa-check"></i><b>2.1</b> Data and dataset types</a></li>
<li class="chapter" data-level="2.2" data-path="preliminaries.html"><a href="preliminaries.html#plots"><i class="fa fa-check"></i><b>2.2</b> Plots</a></li>
<li class="chapter" data-level="2.3" data-path="preliminaries.html"><a href="preliminaries.html#probability-distributions-with-r"><i class="fa fa-check"></i><b>2.3</b> Probability Distributions with R</a></li>
<li class="chapter" data-level="2.4" data-path="preliminaries.html"><a href="preliminaries.html#regressions"><i class="fa fa-check"></i><b>2.4</b> Regressions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="preliminaries.html"><a href="preliminaries.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>2.4.1</b> Ordinary Least Squares (OLS)</a></li>
<li class="chapter" data-level="2.4.2" data-path="preliminaries.html"><a href="preliminaries.html#maximum-likelihood-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="2.4.3" data-path="preliminaries.html"><a href="preliminaries.html#estimating-mle-with-r"><i class="fa fa-check"></i><b>2.4.3</b> Estimating MLE with R</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="preliminaries.html"><a href="preliminaries.html#blue"><i class="fa fa-check"></i><b>2.5</b> BLUE</a></li>
<li class="chapter" data-level="2.6" data-path="preliminaries.html"><a href="preliminaries.html#modeling-the-data"><i class="fa fa-check"></i><b>2.6</b> Modeling the data</a></li>
<li class="chapter" data-level="2.7" data-path="preliminaries.html"><a href="preliminaries.html#causal-vs.-predictive-models"><i class="fa fa-check"></i><b>2.7</b> Causal vs.Â Predictive Models</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="preliminaries.html"><a href="preliminaries.html#causal-models"><i class="fa fa-check"></i><b>2.7.1</b> Causal Models</a></li>
<li class="chapter" data-level="2.7.2" data-path="preliminaries.html"><a href="preliminaries.html#prediction-models"><i class="fa fa-check"></i><b>2.7.2</b> Prediction Models</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="preliminaries.html"><a href="preliminaries.html#simulation"><i class="fa fa-check"></i><b>2.8</b> Simulation</a></li>
</ul></li>
<li class="part"><span><b>I Formal Look at Prediction</b></span></li>
<li class="chapter" data-level="" data-path="learning-systems.html"><a href="learning-systems.html"><i class="fa fa-check"></i>Learning Systems</a></li>
<li class="chapter" data-level="3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html"><i class="fa fa-check"></i><b>3</b> Bias-Variance Tradeoff</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#estimator-and-mse"><i class="fa fa-check"></i><b>3.1</b> Estimator and MSE</a></li>
<li class="chapter" data-level="3.2" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#prediction---mspe"><i class="fa fa-check"></i><b>3.2</b> Prediction - MSPE</a></li>
<li class="chapter" data-level="3.3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>3.3</b> Biased estimator as a predictor</a></li>
<li class="chapter" data-level="3.4" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>3.4</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="3.5" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#uncertainty-in-estimations-and-predictions"><i class="fa fa-check"></i><b>3.5</b> Uncertainty in estimations and predictions</a></li>
<li class="chapter" data-level="3.6" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>3.6</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>4</b> Overfitting</a></li>
<li class="part"><span><b>II Nonparametric Estimations</b></span></li>
<li class="chapter" data-level="" data-path="parametric-vs.html"><a href="parametric-vs.html"><i class="fa fa-check"></i>Parametric vs.Â Nonparametric methods</a></li>
<li class="chapter" data-level="5" data-path="parametric-estimations.html"><a href="parametric-estimations.html"><i class="fa fa-check"></i><b>5</b> Parametric Estimations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="parametric-estimations.html"><a href="parametric-estimations.html#linear-probability-models-lpm"><i class="fa fa-check"></i><b>5.1</b> Linear Probability Models (LPM)</a></li>
<li class="chapter" data-level="5.2" data-path="parametric-estimations.html"><a href="parametric-estimations.html#logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="parametric-estimations.html"><a href="parametric-estimations.html#estimating-logistic-regression"><i class="fa fa-check"></i><b>5.2.1</b> Estimating Logistic Regression</a></li>
<li class="chapter" data-level="5.2.2" data-path="parametric-estimations.html"><a href="parametric-estimations.html#cost-functions"><i class="fa fa-check"></i><b>5.2.2</b> Cost functions</a></li>
<li class="chapter" data-level="5.2.3" data-path="parametric-estimations.html"><a href="parametric-estimations.html#deviance"><i class="fa fa-check"></i><b>5.2.3</b> Deviance</a></li>
<li class="chapter" data-level="5.2.4" data-path="parametric-estimations.html"><a href="parametric-estimations.html#predictive-accuracy"><i class="fa fa-check"></i><b>5.2.4</b> Predictive accuracy</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>6</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="6.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>6.1</b> Density Estimations</a></li>
<li class="chapter" data-level="6.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regressions"><i class="fa fa-check"></i><b>6.2</b> Kernel regressions</a></li>
<li class="chapter" data-level="6.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#regression-splines"><i class="fa fa-check"></i><b>6.3</b> Regression Splines</a></li>
<li class="chapter" data-level="6.4" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#mars---multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>6.4</b> MARS - Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="6.5" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#gam---generalized-additive-model"><i class="fa fa-check"></i><b>6.5</b> GAM - Generalized Additive Model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>7</b> Smoothing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>7.1</b> Using bins</a></li>
<li class="chapter" data-level="7.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>7.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="7.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>7.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="7.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>7.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="7.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>7.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html"><i class="fa fa-check"></i><b>8</b> Nonparametric Classifier - kNN</a>
<ul>
<li class="chapter" data-level="8.1" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#mnist-dataset"><i class="fa fa-check"></i><b>8.1</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="8.2" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#linear-classifiers-again"><i class="fa fa-check"></i><b>8.2</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="8.3" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>8.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="8.4" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#knn-with-caret"><i class="fa fa-check"></i><b>8.4</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#mnist_27"><i class="fa fa-check"></i><b>8.4.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="8.4.2" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#adult-dataset"><i class="fa fa-check"></i><b>8.4.2</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Self-Learning</b></span></li>
<li class="chapter" data-level="9" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>9</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="9.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-validation-and-test-datasets"><i class="fa fa-check"></i><b>9.1</b> Training, validation, and test datasets</a></li>
<li class="chapter" data-level="9.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>9.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="9.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>9.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="9.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#grid-search"><i class="fa fa-check"></i><b>9.4</b> Grid search</a></li>
<li class="chapter" data-level="9.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>9.5</b> Cross-validated grid search</a></li>
<li class="chapter" data-level="9.6" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#when-the-data-is-time-series"><i class="fa fa-check"></i><b>9.6</b> When the data is time-series</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html"><i class="fa fa-check"></i><b>10</b> Tuning in Classification</a>
<ul>
<li class="chapter" data-level="10.1" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>10.1</b> Confusion matrix</a></li>
<li class="chapter" data-level="10.2" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#performance-measures"><i class="fa fa-check"></i><b>10.2</b> Performance measures</a></li>
<li class="chapter" data-level="10.3" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#roc---reciever-operating-curve"><i class="fa fa-check"></i><b>10.3</b> ROC - Reciever Operating Curve</a></li>
<li class="chapter" data-level="10.4" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>10.4</b> AUC - Area Under the Curve</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>11</b> Classification Example</a>
<ul>
<li class="chapter" data-level="11.1" data-path="classification-example.html"><a href="classification-example.html#lpm"><i class="fa fa-check"></i><b>11.1</b> LPM</a></li>
<li class="chapter" data-level="11.2" data-path="classification-example.html"><a href="classification-example.html#logistic-regression-1"><i class="fa fa-check"></i><b>11.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="11.3" data-path="classification-example.html"><a href="classification-example.html#knn"><i class="fa fa-check"></i><b>11.3</b> kNN</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="classification-example.html"><a href="classification-example.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>11.3.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="11.3.2" data-path="classification-example.html"><a href="classification-example.html#knn-with-caret-1"><i class="fa fa-check"></i><b>11.3.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Tree-based Models</b></span></li>
<li class="chapter" data-level="12" data-path="cart.html"><a href="cart.html"><i class="fa fa-check"></i><b>12</b> CART</a>
<ul>
<li class="chapter" data-level="12.1" data-path="cart.html"><a href="cart.html#cart---classification-tree"><i class="fa fa-check"></i><b>12.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="12.2" data-path="cart.html"><a href="cart.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>12.2</b> <code>rpart()</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="12.3" data-path="cart.html"><a href="cart.html#pruning"><i class="fa fa-check"></i><b>12.3</b> Pruning</a></li>
<li class="chapter" data-level="12.4" data-path="cart.html"><a href="cart.html#classification-with-titanic"><i class="fa fa-check"></i><b>12.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="12.5" data-path="cart.html"><a href="cart.html#regression-tree"><i class="fa fa-check"></i><b>12.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ensemble-learning.html"><a href="ensemble-learning.html"><i class="fa fa-check"></i><b>13</b> Ensemble learning</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ensemble-learning.html"><a href="ensemble-learning.html#bagging"><i class="fa fa-check"></i><b>13.1</b> Bagging</a></li>
<li class="chapter" data-level="13.2" data-path="ensemble-learning.html"><a href="ensemble-learning.html#random-forest"><i class="fa fa-check"></i><b>13.2</b> Random Forest</a></li>
<li class="chapter" data-level="13.3" data-path="ensemble-learning.html"><a href="ensemble-learning.html#boosting"><i class="fa fa-check"></i><b>13.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="ensemble-learning.html"><a href="ensemble-learning.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>13.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="13.3.2" data-path="ensemble-learning.html"><a href="ensemble-learning.html#adaboost"><i class="fa fa-check"></i><b>13.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="13.3.3" data-path="ensemble-learning.html"><a href="ensemble-learning.html#extreme-gradient-boosting-xgboost"><i class="fa fa-check"></i><b>13.3.3</b> Extreme Gradient Boosting (XGBoost)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ensemble-applications.html"><a href="ensemble-applications.html"><i class="fa fa-check"></i><b>14</b> Ensemble Applications</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification"><i class="fa fa-check"></i><b>14.1</b> Classification</a></li>
<li class="chapter" data-level="14.2" data-path="ensemble-applications.html"><a href="ensemble-applications.html#regression"><i class="fa fa-check"></i><b>14.2</b> Regression</a></li>
<li class="chapter" data-level="14.3" data-path="ensemble-applications.html"><a href="ensemble-applications.html#dataset-level-explainers"><i class="fa fa-check"></i><b>14.3</b> Dataset-level explainers</a></li>
<li class="chapter" data-level="14.4" data-path="ensemble-applications.html"><a href="ensemble-applications.html#boosting-applications"><i class="fa fa-check"></i><b>14.4</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="ensemble-applications.html"><a href="ensemble-applications.html#regression-1"><i class="fa fa-check"></i><b>14.4.1</b> Regression</a></li>
<li class="chapter" data-level="14.4.2" data-path="ensemble-applications.html"><a href="ensemble-applications.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>14.4.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="14.4.3" data-path="ensemble-applications.html"><a href="ensemble-applications.html#boosting-vs.-others"><i class="fa fa-check"></i><b>14.4.3</b> Boosting vs.Â Others</a></li>
<li class="chapter" data-level="14.4.4" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification-1"><i class="fa fa-check"></i><b>14.4.4</b> Classification</a></li>
<li class="chapter" data-level="14.4.5" data-path="ensemble-applications.html"><a href="ensemble-applications.html#adaboost.m1"><i class="fa fa-check"></i><b>14.4.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="14.4.6" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification-with-xgboost"><i class="fa fa-check"></i><b>14.4.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V SVM &amp; Neural Networks</b></span></li>
<li class="chapter" data-level="15" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>15</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="15.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>15.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-margin"><i class="fa fa-check"></i><b>15.1.1</b> The Margin</a></li>
<li class="chapter" data-level="15.1.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-non-separable-case"><i class="fa fa-check"></i><b>15.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>15.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="15.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#application-with-svm"><i class="fa fa-check"></i><b>15.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>16</b> Neural Networks</a></li>
<li class="part"><span><b>VI Penalized Regressions</b></span></li>
<li class="chapter" data-level="" data-path="parametric-models-in-prediction.html"><a href="parametric-models-in-prediction.html"><i class="fa fa-check"></i>Parametric models in prediction</a></li>
<li class="chapter" data-level="17" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>17</b> Ridge</a></li>
<li class="chapter" data-level="18" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>18</b> Lasso</a>
<ul>
<li class="chapter" data-level="18.1" data-path="lasso.html"><a href="lasso.html#regression-2"><i class="fa fa-check"></i><b>18.1</b> Regression</a></li>
<li class="chapter" data-level="18.2" data-path="lasso.html"><a href="lasso.html#lpmlogistic"><i class="fa fa-check"></i><b>18.2</b> LPM/Logistic</a></li>
<li class="chapter" data-level="18.3" data-path="lasso.html"><a href="lasso.html#adaptive-lasso"><i class="fa fa-check"></i><b>18.3</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="18.4" data-path="lasso.html"><a href="lasso.html#elasticnet"><i class="fa fa-check"></i><b>18.4</b> ElasticNet</a></li>
</ul></li>
<li class="part"><span><b>VII Causality and Machine Learning</b></span></li>
<li class="chapter" data-level="19" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>19</b> Model Selection</a></li>
<li class="chapter" data-level="20" data-path="sparsity.html"><a href="sparsity.html"><i class="fa fa-check"></i><b>20</b> Sparsity</a></li>
<li class="chapter" data-level="21" data-path="use-of-machine-learning-in-causality.html"><a href="use-of-machine-learning-in-causality.html"><i class="fa fa-check"></i><b>21</b> Use of Machine Learning in Causality</a></li>
<li class="part"><span><b>VIII Dimension Reduction Methods</b></span></li>
<li class="chapter" data-level="" data-path="matrix-decompositions.html"><a href="matrix-decompositions.html"><i class="fa fa-check"></i>Matrix Decompositions</a></li>
<li class="chapter" data-level="22" data-path="eigenvectors-and-eigenvalues.html"><a href="eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>22</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="23" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html"><i class="fa fa-check"></i><b>23</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="24" data-path="rankr-approximations.html"><a href="rankr-approximations.html"><i class="fa fa-check"></i><b>24</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="25" data-path="moore-penrose-inverse.html"><a href="moore-penrose-inverse.html"><i class="fa fa-check"></i><b>25</b> Moore-Penrose inverse</a></li>
<li class="chapter" data-level="26" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html"><i class="fa fa-check"></i><b>26</b> Principle Component Analysis</a></li>
<li class="chapter" data-level="27" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>27</b> Factor Analysis</a></li>
<li class="chapter" data-level="28" data-path="dynamic-mode-decomposition.html"><a href="dynamic-mode-decomposition.html"><i class="fa fa-check"></i><b>28</b> Dynamic Mode Decomposition</a></li>
<li class="part"><span><b>IX Time Series</b></span></li>
<li class="chapter" data-level="29" data-path="arima-models.html"><a href="arima-models.html"><i class="fa fa-check"></i><b>29</b> ARIMA models</a></li>
<li class="chapter" data-level="30" data-path="grid-search-for-arima.html"><a href="grid-search-for-arima.html"><i class="fa fa-check"></i><b>30</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="31" data-path="embedding.html"><a href="embedding.html"><i class="fa fa-check"></i><b>31</b> Embedding</a></li>
<li class="chapter" data-level="32" data-path="ts-with-random-forest.html"><a href="ts-with-random-forest.html"><i class="fa fa-check"></i><b>32</b> TS with Random Forest</a></li>
<li class="chapter" data-level="33" data-path="neural-networks-ts.html"><a href="neural-networks-ts.html"><i class="fa fa-check"></i><b>33</b> Neural Networks &amp; TS</a></li>
<li class="part"><span><b>X Network Analysis</b></span></li>
<li class="chapter" data-level="" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i>Graphical Network Analysis</a></li>
<li class="chapter" data-level="34" data-path="fundementals.html"><a href="fundementals.html"><i class="fa fa-check"></i><b>34</b> Fundementals</a>
<ul>
<li class="chapter" data-level="34.1" data-path="fundementals.html"><a href="fundementals.html#covariance"><i class="fa fa-check"></i><b>34.1</b> Covariance</a></li>
<li class="chapter" data-level="34.2" data-path="fundementals.html"><a href="fundementals.html#correlation"><i class="fa fa-check"></i><b>34.2</b> Correlation</a></li>
<li class="chapter" data-level="34.3" data-path="fundementals.html"><a href="fundementals.html#precision-matrix"><i class="fa fa-check"></i><b>34.3</b> Precision matrix</a></li>
<li class="chapter" data-level="34.4" data-path="fundementals.html"><a href="fundementals.html#semi-partial-correlation"><i class="fa fa-check"></i><b>34.4</b> Semi-partial correlation</a></li>
</ul></li>
<li class="chapter" data-level="35" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html"><i class="fa fa-check"></i><b>35</b> Regularized covariance matrix</a>
<ul>
<li class="chapter" data-level="35.1" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#mle"><i class="fa fa-check"></i><b>35.1</b> MLE</a></li>
<li class="chapter" data-level="35.2" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#high-dimensional-data"><i class="fa fa-check"></i><b>35.2</b> High-dimensional data</a></li>
<li class="chapter" data-level="35.3" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>35.3</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
<li class="chapter" data-level="35.4" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#whats-graphical---graphical-ridge-or-glasso"><i class="fa fa-check"></i><b>35.4</b> Whatâs graphical - graphical ridge or glasso?</a></li>
</ul></li>
<li class="part"><span><b>XI Labs</b></span></li>
<li class="chapter" data-level="36" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html"><i class="fa fa-check"></i><b>36</b> R Lab 1 - Basics I</a>
<ul>
<li class="chapter" data-level="36.1" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#r-rstudio-and-r-packages"><i class="fa fa-check"></i><b>36.1</b> R, RStudio, and R Packages</a></li>
<li class="chapter" data-level="36.2" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#rstudio"><i class="fa fa-check"></i><b>36.2</b> RStudio</a></li>
<li class="chapter" data-level="36.3" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#working-directory"><i class="fa fa-check"></i><b>36.3</b> Working directory</a></li>
<li class="chapter" data-level="36.4" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#data-types-and-stuctures"><i class="fa fa-check"></i><b>36.4</b> Data Types and Stuctures</a></li>
<li class="chapter" data-level="36.5" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#vectors"><i class="fa fa-check"></i><b>36.5</b> Vectors</a></li>
<li class="chapter" data-level="36.6" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#subsetting-vectors"><i class="fa fa-check"></i><b>36.6</b> Subsetting Vectors</a></li>
<li class="chapter" data-level="36.7" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#vectorization-or-vector-operations"><i class="fa fa-check"></i><b>36.7</b> Vectorization or vector operations</a></li>
<li class="chapter" data-level="36.8" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#matrices"><i class="fa fa-check"></i><b>36.8</b> Matrices</a></li>
<li class="chapter" data-level="36.9" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#matrix-operations"><i class="fa fa-check"></i><b>36.9</b> Matrix Operations</a></li>
<li class="chapter" data-level="36.10" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#subsetting-matrix"><i class="fa fa-check"></i><b>36.10</b> Subsetting Matrix</a></li>
<li class="chapter" data-level="36.11" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#r-style-guide"><i class="fa fa-check"></i><b>36.11</b> R-Style Guide</a></li>
</ul></li>
<li class="chapter" data-level="37" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html"><i class="fa fa-check"></i><b>37</b> R Lab 2 - Basics II</a>
<ul>
<li class="chapter" data-level="37.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#data-frames-and-lists"><i class="fa fa-check"></i><b>37.1</b> Data frames and lists</a>
<ul>
<li class="chapter" data-level="37.1.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#lists"><i class="fa fa-check"></i><b>37.1.1</b> Lists</a></li>
<li class="chapter" data-level="37.1.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#data-frames"><i class="fa fa-check"></i><b>37.1.2</b> Data Frames</a></li>
<li class="chapter" data-level="37.1.3" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#reading-importing-and-writting-exporting-data-files"><i class="fa fa-check"></i><b>37.1.3</b> Reading (importing) and writting (exporting) data files</a></li>
<li class="chapter" data-level="37.1.4" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#subsetting-data-frames"><i class="fa fa-check"></i><b>37.1.4</b> Subsetting Data Frames</a></li>
<li class="chapter" data-level="37.1.5" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#plotting-from-data-frame"><i class="fa fa-check"></i><b>37.1.5</b> Plotting from data frame</a></li>
<li class="chapter" data-level="37.1.6" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#some-useful-functions"><i class="fa fa-check"></i><b>37.1.6</b> Some useful functions</a></li>
<li class="chapter" data-level="37.1.7" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#categorical-variables-in-data-frames"><i class="fa fa-check"></i><b>37.1.7</b> Categorical Variables in Data Frames</a></li>
</ul></li>
<li class="chapter" data-level="37.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#programming-basics"><i class="fa fa-check"></i><b>37.2</b> Programming Basics</a>
<ul>
<li class="chapter" data-level="37.2.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#ifelse"><i class="fa fa-check"></i><b>37.2.1</b> if/Else</a></li>
<li class="chapter" data-level="37.2.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#loops"><i class="fa fa-check"></i><b>37.2.2</b> Loops</a></li>
<li class="chapter" data-level="37.2.3" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#the-apply-family"><i class="fa fa-check"></i><b>37.2.3</b> The <code>apply()</code> family</a></li>
<li class="chapter" data-level="37.2.4" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#functions"><i class="fa fa-check"></i><b>37.2.4</b> Functions</a></li>
<li class="chapter" data-level="37.2.5" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#dplyr"><i class="fa fa-check"></i><b>37.2.5</b> <code>dplyr()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="38" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html"><i class="fa fa-check"></i><b>38</b> R Lab 3 - Preparing the data</a>
<ul>
<li class="chapter" data-level="38.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#preparing-the-data-for-a-regression-analysis-with-lm"><i class="fa fa-check"></i><b>38.1</b> Preparing the data for a regression analysis with <code>lm()</code></a>
<ul>
<li class="chapter" data-level="38.1.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#factor-variables"><i class="fa fa-check"></i><b>38.1.1</b> Factor variables</a></li>
<li class="chapter" data-level="38.1.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#dummy-coding"><i class="fa fa-check"></i><b>38.1.2</b> Dummy Coding</a></li>
<li class="chapter" data-level="38.1.3" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#column-variable-names"><i class="fa fa-check"></i><b>38.1.3</b> Column (Variable) names</a></li>
<li class="chapter" data-level="38.1.4" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#data-subsetting-and-missing-values"><i class="fa fa-check"></i><b>38.1.4</b> Data subsetting and missing values</a></li>
</ul></li>
<li class="chapter" data-level="38.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#dummy-variable-models"><i class="fa fa-check"></i><b>38.2</b> âDUMMYâ variable models</a>
<ul>
<li class="chapter" data-level="38.2.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#mtcars-example"><i class="fa fa-check"></i><b>38.2.1</b> <code>mtcars</code> example</a></li>
<li class="chapter" data-level="38.2.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#model.matrix"><i class="fa fa-check"></i><b>38.2.2</b> <code>model.matrix()</code></a></li>
<li class="chapter" data-level="38.2.3" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#example-with-a-bigger-data-set-autompg"><i class="fa fa-check"></i><b>38.2.3</b> Example with a bigger data set: <code>Autompg</code></a></li>
<li class="chapter" data-level="38.2.4" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#some-more-data-management-tools-for-subsetting-complete.cases-is.na-and-within"><i class="fa fa-check"></i><b>38.2.4</b> Some more data management tools for subsetting: <code>complete.cases()</code>, <code>is.na()</code>, and <code>within()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="39" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html"><i class="fa fa-check"></i><b>39</b> R Lab 4 - Simulation in R</a>
<ul>
<li class="chapter" data-level="39.1" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#sampling-in-r-sample"><i class="fa fa-check"></i><b>39.1</b> Sampling in R: <code>sample()</code></a></li>
<li class="chapter" data-level="39.2" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#random-number-generating-with-probablity-distributions"><i class="fa fa-check"></i><b>39.2</b> Random number generating with probablity distributions</a></li>
<li class="chapter" data-level="39.3" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#simulation-for-statistical-inference"><i class="fa fa-check"></i><b>39.3</b> Simulation for statistical inference</a></li>
<li class="chapter" data-level="39.4" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#creataing-data-with-a-data-generating-model-dgm"><i class="fa fa-check"></i><b>39.4</b> Creataing data with a Data Generating Model (DGM)</a></li>
<li class="chapter" data-level="39.5" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#bootstrapping"><i class="fa fa-check"></i><b>39.5</b> Bootstrapping</a></li>
<li class="chapter" data-level="39.6" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#monty-hall---fun-example"><i class="fa fa-check"></i><b>39.6</b> Monty Hall - Fun example</a></li>
</ul></li>
<li class="part"><span><b>XII Appendix</b></span></li>
<li class="chapter" data-level="40" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html"><i class="fa fa-check"></i><b>40</b> Algorithmic Optimization</a>
<ul>
<li class="chapter" data-level="40.1" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#brute-force-optimization"><i class="fa fa-check"></i><b>40.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="40.2" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#derivative-based-methods"><i class="fa fa-check"></i><b>40.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="40.3" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>40.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="40.4" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#gradient-descent"><i class="fa fa-check"></i><b>40.4</b> Gradient Descent</a>
<ul>
<li class="chapter" data-level="40.4.1" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#one-variable"><i class="fa fa-check"></i><b>40.4.1</b> One-variable</a></li>
<li class="chapter" data-level="40.4.2" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#multivariable"><i class="fa fa-check"></i><b>40.4.2</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="40.5" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#optimization-with-r"><i class="fa fa-check"></i><b>40.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="41" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>41</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="41.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>41.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="41.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>41.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/yaydede/ToolShed" target="blank">&copy; 2020 - 2022 Yigit Aydede</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Toolbox for Social Scientists and Policy Analysts</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regularized-covariance-matrix" class="section level1 hasAnchor" number="35">
<h1><span class="header-section-number">Chapter 35</span> Regularized covariance matrix<a href="regularized-covariance-matrix.html#regularized-covariance-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Why is a covariance matrix <span class="math inline">\(S\)</span> is singular when <span class="math inline">\(n&lt;p\)</span> in the data matrix of <span class="math inline">\(X\)</span>? Consider the <span class="math inline">\(n \times p\)</span> matrix of sample data, <span class="math inline">\(X\)</span>. From the above, the rank of <span class="math inline">\(X\)</span> is at most <span class="math inline">\(\min (n, p)\)</span>. Since</p>
<p><span class="math display">\[
\mathbf{S}=\frac{1}{n} \mathbf{X}_{c}^{\prime} \mathbf{X}_{c}
\]</span>
<span class="math inline">\(\operatorname{rank}(X_c)\)</span> will be <span class="math inline">\(n\)</span> (<span class="math inline">\(n&lt;p\)</span>). Since <span class="math inline">\(\operatorname{rank}(A B) \leq \min (\operatorname{rank}(A), \operatorname{rank}(B))\)</span>. Clearly the rank of <span class="math inline">\(S\)</span> wonât be larger than the rank of <span class="math inline">\(X_c\)</span>. Since <span class="math inline">\(S\)</span> is <span class="math inline">\(p \times p\)</span> and its rank is <span class="math inline">\(n\)</span>, <span class="math inline">\(S\)</span> will be singular. Thatâs, if <span class="math inline">\(n&lt;p\)</span> then <span class="math inline">\(\operatorname{rank}(X)&lt;p\)</span> in which case <span class="math inline">\(\operatorname{rank}(S)&lt;p\)</span>.</p>
<div id="mle" class="section level2 hasAnchor" number="35.1">
<h2><span class="header-section-number">35.1</span> MLE<a href="regularized-covariance-matrix.html#mle" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before understanding L1 or L2 regularization, we need to see the multivariate Gaussian distribution, its parameterization and MLE solutions. The multivariate Gaussian distribution of a random vector <span class="math inline">\(X \in \mathbf{R}^{p}\)</span> is commonly expressed in terms of the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span>, where <span class="math inline">\(\mu\)</span> is an <span class="math inline">\(p \times 1\)</span> vector and <span class="math inline">\(\Sigma\)</span> is an <span class="math inline">\(p \times p\)</span>, a nonsingular symmetric covariance matrix. Hence, we have the following form for the density function (the Wishart distribution arises as the distribution of the sample covariance matrix for a sample from a multivariate normal distribution - <a href="https://en.wikipedia.org/wiki/Wishart_distribution">See Wishard Distribution</a>):</p>
<p><span class="math display">\[
f(x \mid \mu, \Sigma)=\frac{1}{(2 \pi)^{p / 2}|\Sigma|^{1 / 2}} \exp \left\{-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right\},
\]</span>
where <span class="math inline">\(|\Sigma|\)</span> is the determinant of the covariance matrix. The likelihood function is:</p>
<p><span class="math display">\[
\mathcal{L}(\mu, \Sigma)=(2 \pi)^{-\frac{n p}{2}} \prod_{i=1}^{n} \operatorname{det}(\Sigma)^{-\frac{1}{2}} \exp \left(-\frac{1}{2}\left(x_{i}-\mu\right)^{\mathrm{T}} \Sigma^{-1}\left(x_{i}-\mu\right)\right)
\]</span>
Since the estimate <span class="math inline">\(\bar{x}\)</span> does not depend on <span class="math inline">\(\Sigma\)</span>, we can just substitute it for <span class="math inline">\(\mu\)</span> in the likelihood function, getting</p>
<p><span class="math display">\[
\mathcal{L}(\bar{x}, \Sigma) \propto \operatorname{det}(\Sigma)^{-\frac{n}{2}} \exp \left(-\frac{1}{2} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \Sigma^{-1}\left(x_{i}-\bar{x}\right)\right)
\]</span></p>
<p>and then seek the value of <span class="math inline">\(\Sigma\)</span> that maximizes the likelihood of the data (in practice it is easier to work with <span class="math inline">\(\log \mathcal{L}\)</span> ). Regard the scalar <span class="math inline">\(\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \Sigma^{-1}\left(x_{i}-\bar{x}\right)\)</span> as the trace of a <span class="math inline">\(1 \times 1\)</span> matrix. This makes it possible to use the identity <span class="math inline">\(\operatorname{tr}(A B)=\operatorname{tr}(B A)\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\mathcal{L}(\bar{x}, \Sigma) &amp; \propto \operatorname{det}(\Sigma)^{-\frac{n}{2}} \exp \left(-\frac{1}{2} \sum_{i=1}^{n}\left(\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \Sigma^{-1}\left(x_{i}-\bar{x}\right)\right)\right) \\
&amp;=\operatorname{det}(\Sigma)^{-\frac{n}{2}} \exp \left(-\frac{1}{2} \sum_{i=1}^{n} \operatorname{tr}\left(\left(x_{i}-\bar{x}\right)\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \Sigma^{-1}\right)\right) \\
&amp;=\operatorname{det}(\Sigma)^{-\frac{n}{2}} \exp \left(-\frac{1}{2} \operatorname{tr}\left(\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \Sigma^{-1}\right)\right) \\
&amp;=\operatorname{det}(\Sigma)^{-\frac{n}{2}} \exp \left(-\frac{1}{2} \operatorname{tr}\left(S \Sigma^{-1}\right)\right)
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
S=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \in \mathbf{R}^{p \times p}
\]</span>
And finally, re-write the likelihood in the log form using the trace trick:</p>
<p><span class="math display">\[
\ln \mathcal{L}(\mu, \Sigma)=\text { const }-\frac{n}{2} \ln \operatorname{det}(\Sigma)-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1} \sum_{i=1}^{n}\left(x_{i}-\mu\right)\left(x_{i}-\mu\right)^{\mathrm{T}}\right]
\]</span></p>
<p>or, for a multivariate normal model with mean 0 and covariance <span class="math inline">\(\Sigma\)</span>, the likelihood function in this case is given by</p>
<p><span class="math display">\[
\ell(\Omega ; S)=\ln |\Omega|-\operatorname{tr}(S \Omega)
\]</span></p>
<p>where <span class="math inline">\(\Omega=\Sigma^{-1}\)</span> is the so-called precision matrix (also sometimes called the concentration matrix). It is precisely this <span class="math inline">\(\Omega\)</span> for which we seek an estimate, which we will denote <span class="math inline">\(P\)</span>. Indeed, one can naturally try to use the inverse of <span class="math inline">\(S\)</span> for this.</p>
<p>The differential of this log-likelihood is</p>
<p><span class="math display">\[
d \ln \mathcal{L}(\mu, \Sigma)=\\-\frac{n}{2} \operatorname{tr}\left[\Sigma^{-1}\{d \Sigma\}\right]-\frac{1}{2} \operatorname{tr}\left[-\Sigma^{-1}\{d \Sigma\} \Sigma^{-1} \sum_{i=1}^{n}\left(x_{i}-\mu\right)\left(x_{i}-\mu\right)^{\mathrm{T}}-2 \Sigma^{-1} \sum_{i=1}^{n}\left(x_{i}-\mu\right)\{d \mu\}^{\mathrm{T}}\right]
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\sum_{i=1}^{n}\left(x_{i}-\mu\right)\left(x_{i}-\mu\right)^{\mathrm{T}}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(x_{i}-\bar{x}\right)^{\mathrm{T}}=S
\]</span>
Then the terms involving <span class="math inline">\(d \Sigma\)</span> in <span class="math inline">\(d \ln \mathcal{L}\)</span> can be combined as</p>
<p><span class="math display">\[
-\frac{1}{2} \operatorname{tr}\left(\Sigma^{-1}\{d \Sigma\}\left[n I_{p}-\Sigma^{-1} S\right]\right)
\]</span>
See the rest from <a href="https://en.wikipedia.org/wiki/Estimation_of_covariance_matrices" class="uri">https://en.wikipedia.org/wiki/Estimation_of_covariance_matrices</a>. For <span class="math inline">\(n &lt; p\)</span>, the empirical estimate of the covariance matrix becomes singular, i.e.Â it cannot be inverted to compute the precision matrix.</p>
<p>There is also another intuitive way to see the whole algebra (<a href="https://stats.stackexchange.com/questions/151315/what-is-the-intuitive-geometric-meaning-of-minimizing-the-log-determinant-of">see the post here</a>) <span class="citation">(<a href="#ref-Intui_Cross" role="doc-biblioref">Lepidopterist 2015</a>)</span>:</p>
<p>Letâs start with the univariate standard normal density (parameter free) which is
<span class="math display">\[
\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{1}{2} t^{2}\right)
\]</span>
When we extend (parameterize) it to <span class="math inline">\(x=\sigma t+\mu\)</span>, the change of variable requires <span class="math inline">\(d t=\frac{1}{\sigma} d x\)</span> making the general normal density
<span class="math display">\[
\frac{1}{\sqrt{2 \pi}} \frac{1}{\sigma} \exp \left(-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right)
\]</span>
The log-likelihood is
<span class="math display">\[
\text { A constant }-\frac{n}{2} \log \left(\sigma^{2}\right)-\frac{1}{2} \sum_{i=1}^{n}\left(\frac{x_{i}-\mu}{\sigma}\right)^{2},
\]</span>
maximization of which is equivalent to minimizing
<span class="math display">\[
n \log \left(\sigma^{2}\right)+\sum_{i=1}^{n}\left(\frac{x_{i}-\mu}{\sigma}\right)^{2}
\]</span></p>
<p>Multivariate (say number of dimensions <span class="math inline">\(=d\)</span> ) counterpart behaves the similar way. Starting with the generating (standard) density
<span class="math display">\[
(\sqrt{2 \pi})^{-d} \exp \left(-\frac{1}{2} \mathbf{z}^{t} \mathbf{z}\right)
\]</span>
and the general multivariate normal (MVN) density is
<span class="math display">\[
(\sqrt{2 \pi})^{-d}|\boldsymbol{\Sigma}|^{-1 / 2} \exp \left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{t} \mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)
\]</span></p>
<p>Observe that <span class="math inline">\(|\boldsymbol{\Sigma}|^{-1 / 2}\)</span> (which is the reciprocal of the square root of the determinant of the covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> ) in the multivariate case does what <span class="math inline">\(1 / \sigma\)</span> does in the univariate case and <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> does what <span class="math inline">\(1 / \sigma^{2}\)</span> does in the univariate case. In simpler terms, <span class="math inline">\(|\boldsymbol{\Sigma}|^{-1 / 2}\)</span> is the change of variable âadjustmentâ.
The maximization of likelihood would lead to minimizing (analogous to the univariate case)</p>
<p><span class="math display">\[
n \log |\boldsymbol{\Sigma}|+\sum_{i=1}^{n}(\mathbf{x}-\boldsymbol{\mu})^{t} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})
\]</span></p>
<p>Again, in simpler terms, <span class="math inline">\(n \log |\mathbf{\Sigma}|\)</span> takes the spot of <span class="math inline">\(n \log \left(\sigma^{2}\right)\)</span> which was there in the univariate case. These terms account for corresponding change of variable adjustments in each scenario.</p>
<p>Letâs start with a data matrix of 10x6, where no need for regularization.</p>
<div class="sourceCode" id="cb990"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb990-1"><a href="regularized-covariance-matrix.html#cb990-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb990-2"><a href="regularized-covariance-matrix.html#cb990-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="dv">6</span></span>
<span id="cb990-3"><a href="regularized-covariance-matrix.html#cb990-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span> (<span class="fu">rnorm</span>(n<span class="sc">*</span>p), n, p)</span>
<span id="cb990-4"><a href="regularized-covariance-matrix.html#cb990-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb990-5"><a href="regularized-covariance-matrix.html#cb990-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Cov. &amp; Precision Matrices</span></span>
<span id="cb990-6"><a href="regularized-covariance-matrix.html#cb990-6" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">cov</span>(X)</span>
<span id="cb990-7"><a href="regularized-covariance-matrix.html#cb990-7" aria-hidden="true" tabindex="-1"></a>pm <span class="ot">&lt;-</span> <span class="fu">solve</span>(S) <span class="co"># precision</span></span>
<span id="cb990-8"><a href="regularized-covariance-matrix.html#cb990-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb990-9"><a href="regularized-covariance-matrix.html#cb990-9" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span>pm[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span>(<span class="fu">sqrt</span>(pm[<span class="dv">1</span>,<span class="dv">1</span>])<span class="sc">*</span><span class="fu">sqrt</span>(pm[<span class="dv">2</span>,<span class="dv">2</span>])) </span></code></pre></div>
<pre><code>## [1] -0.6059634</code></pre>
<div class="sourceCode" id="cb992"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb992-1"><a href="regularized-covariance-matrix.html#cb992-1" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span><span class="fu">cov2cor</span>(pm)</span></code></pre></div>
<pre><code>##             [,1]       [,2]       [,3]       [,4]       [,5]        [,6]
## [1,] -1.00000000 -0.6059634  0.1947369 -0.4292779  0.3289393  0.06004998
## [2,] -0.60596342 -1.0000000 -0.3477352 -0.7813197  0.6814685  0.50178834
## [3,]  0.19473691 -0.3477352 -1.0000000 -0.1892090  0.0775629  0.40192145
## [4,] -0.42927791 -0.7813197 -0.1892090 -1.0000000  0.8678627  0.27524750
## [5,]  0.32893932  0.6814685  0.0775629  0.8678627 -1.0000000 -0.39719667
## [6,]  0.06004998  0.5017883  0.4019214  0.2752475 -0.3971967 -1.00000000</code></pre>
<div class="sourceCode" id="cb994"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb994-1"><a href="regularized-covariance-matrix.html#cb994-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ppcor</span></span>
<span id="cb994-2"><a href="regularized-covariance-matrix.html#cb994-2" aria-hidden="true" tabindex="-1"></a>pc <span class="ot">&lt;-</span> ppcor<span class="sc">::</span><span class="fu">pcor</span>(X)</span>
<span id="cb994-3"><a href="regularized-covariance-matrix.html#cb994-3" aria-hidden="true" tabindex="-1"></a>pc<span class="sc">$</span>estimate</span></code></pre></div>
<pre><code>##             [,1]       [,2]       [,3]       [,4]       [,5]        [,6]
## [1,]  1.00000000 -0.6059634  0.1947369 -0.4292779  0.3289393  0.06004998
## [2,] -0.60596342  1.0000000 -0.3477352 -0.7813197  0.6814685  0.50178834
## [3,]  0.19473691 -0.3477352  1.0000000 -0.1892090  0.0775629  0.40192145
## [4,] -0.42927791 -0.7813197 -0.1892090  1.0000000  0.8678627  0.27524750
## [5,]  0.32893932  0.6814685  0.0775629  0.8678627  1.0000000 -0.39719667
## [6,]  0.06004998  0.5017883  0.4019214  0.2752475 -0.3971967  1.00000000</code></pre>
<div class="sourceCode" id="cb996"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb996-1"><a href="regularized-covariance-matrix.html#cb996-1" aria-hidden="true" tabindex="-1"></a><span class="co"># glasso</span></span>
<span id="cb996-2"><a href="regularized-covariance-matrix.html#cb996-2" aria-hidden="true" tabindex="-1"></a>glassoFast<span class="sc">::</span><span class="fu">glassoFast</span>(S,<span class="at">rho=</span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>## $w
##             [,1]        [,2]       [,3]        [,4]        [,5]       [,6]
## [1,]  0.60140347 -0.37332949  0.3171606  0.02177682 -0.13790645 -0.1327159
## [2,] -0.37332949  0.50242861 -0.2405377 -0.25645323  0.01832675  0.2601432
## [3,]  0.31716062 -0.24053766  0.5735265 -0.09239610 -0.28453786  0.1190710
## [4,]  0.02177682 -0.25645323 -0.0923961  0.66021163  0.65364259 -0.3052422
## [5,] -0.13790645  0.01832675 -0.2845379  0.65364259  1.10471906 -0.3465787
## [6,] -0.13271587  0.26014323  0.1190710 -0.30524217 -0.34657873  0.7010335
## 
## $wi
##            [,1]      [,2]       [,3]      [,4]       [,5]       [,6]
## [1,]  4.3912715  4.562946 -0.7541076  2.932966 -1.5229061 -0.2098044
## [2,]  4.5629462 12.914469  2.3082064  9.154998 -5.4111089 -3.0096715
## [3,] -0.7541076  2.308206  3.4125042  1.139186 -0.3163017 -1.2392019
## [4,]  2.9329659  9.154998  1.1391865 10.631981 -6.2528743 -1.4976793
## [5,] -1.5229061 -5.411109 -0.3163017 -6.252874  4.8825940  1.4647642
## [6,] -0.2098044 -3.009672 -1.2392019 -1.497679  1.4647642  2.7860635
## 
## $errflag
## [1] 0
## 
## $niter
## [1] 1</code></pre>
<div class="sourceCode" id="cb998"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb998-1"><a href="regularized-covariance-matrix.html#cb998-1" aria-hidden="true" tabindex="-1"></a>Rl <span class="ot">&lt;-</span> glassoFast<span class="sc">::</span><span class="fu">glassoFast</span>(S,<span class="at">rho=</span><span class="dv">0</span>)<span class="sc">$</span>wi <span class="co">#</span></span>
<span id="cb998-2"><a href="regularized-covariance-matrix.html#cb998-2" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span>Rl[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span>(<span class="fu">sqrt</span>(Rl[<span class="dv">1</span>,<span class="dv">1</span>])<span class="sc">*</span><span class="fu">sqrt</span>(Rl[<span class="dv">2</span>,<span class="dv">2</span>])) </span></code></pre></div>
<pre><code>## [1] -0.6059153</code></pre>
<div class="sourceCode" id="cb1000"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1000-1"><a href="regularized-covariance-matrix.html#cb1000-1" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span><span class="fu">cov2cor</span>(Rl)</span></code></pre></div>
<pre><code>##             [,1]       [,2]        [,3]       [,4]        [,5]        [,6]
## [1,] -1.00000000 -0.6059153  0.19480566 -0.4292445  0.32889151  0.05998241
## [2,] -0.60591532 -1.0000000 -0.34769605 -0.7812911  0.68143226  0.50174762
## [3,]  0.19480566 -0.3476961 -1.00000000 -0.1891260  0.07748893  0.40189255
## [4,] -0.42924454 -0.7812911 -0.18912595 -1.0000000  0.86785527  0.27517959
## [5,]  0.32889151  0.6814323  0.07748893  0.8678553 -1.00000000 -0.39714298
## [6,]  0.05998241  0.5017476  0.40189255  0.2751796 -0.39714298 -1.00000000</code></pre>
</div>
<div id="high-dimensional-data" class="section level2 hasAnchor" number="35.2">
<h2><span class="header-section-number">35.2</span> High-dimensional data<a href="regularized-covariance-matrix.html#high-dimensional-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now with a data matrix of 6x10:</p>
<div class="sourceCode" id="cb1002"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1002-1"><a href="regularized-covariance-matrix.html#cb1002-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">6</span></span>
<span id="cb1002-2"><a href="regularized-covariance-matrix.html#cb1002-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb1002-3"><a href="regularized-covariance-matrix.html#cb1002-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb1002-4"><a href="regularized-covariance-matrix.html#cb1002-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span> (<span class="fu">rnorm</span>(n<span class="sc">*</span>p), n, p)</span>
<span id="cb1002-5"><a href="regularized-covariance-matrix.html#cb1002-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1002-6"><a href="regularized-covariance-matrix.html#cb1002-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Cov. &amp; Precision Matrices</span></span>
<span id="cb1002-7"><a href="regularized-covariance-matrix.html#cb1002-7" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">cov</span>(X)</span>
<span id="cb1002-8"><a href="regularized-covariance-matrix.html#cb1002-8" aria-hidden="true" tabindex="-1"></a>S</span></code></pre></div>
<pre><code>##               [,1]        [,2]        [,3]        [,4]         [,5]        [,6]
##  [1,]  0.889211221 -0.17223814 -0.36660043  0.35320957 -0.629545741 -0.27978848
##  [2,] -0.172238139  0.34416306 -0.09280183 -0.04282613  0.139236591 -0.26060435
##  [3,] -0.366600426 -0.09280183  1.46701338 -0.50796342 -0.024550727 -0.11504405
##  [4,]  0.353209573 -0.04282613 -0.50796342  1.24117592 -0.292005017  0.42646139
##  [5,] -0.629545741  0.13923659 -0.02455073 -0.29200502  0.553562287  0.26275658
##  [6,] -0.279788479 -0.26060435 -0.11504405  0.42646139  0.262756584  0.81429052
##  [7,]  0.143364328 -0.14895377  0.29598156  0.30839120 -0.275296303  0.04418159
##  [8,] -0.273835576  0.17201439 -0.31052657 -0.39667581  0.376175973 -0.02536104
##  [9,] -0.008919669  0.24390178 -0.50198614  0.52741301  0.008044799 -0.01297542
## [10,] -0.304722895  0.33936685 -1.08854590  0.20441696  0.499437080  0.20218868
##              [,7]        [,8]         [,9]      [,10]
##  [1,]  0.14336433 -0.27383558 -0.008919669 -0.3047229
##  [2,] -0.14895377  0.17201439  0.243901782  0.3393668
##  [3,]  0.29598156 -0.31052657 -0.501986137 -1.0885459
##  [4,]  0.30839120 -0.39667581  0.527413006  0.2044170
##  [5,] -0.27529630  0.37617597  0.008044799  0.4994371
##  [6,]  0.04418159 -0.02536104 -0.012975416  0.2021887
##  [7,]  0.37576405 -0.40476558  0.046294293 -0.4691147
##  [8,] -0.40476558  0.46612332 -0.026813818  0.5588965
##  [9,]  0.04629429 -0.02681382  0.540956259  0.5036908
## [10,] -0.46911465  0.55889647  0.503690786  1.3107637</code></pre>
<div class="sourceCode" id="cb1004"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1004-1"><a href="regularized-covariance-matrix.html#cb1004-1" aria-hidden="true" tabindex="-1"></a><span class="fu">try</span>(<span class="fu">solve</span>(S), <span class="at">silent =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## Error in solve.default(S) : 
##   system is computationally singular: reciprocal condition number = 3.99819e-19</code></pre>
<p>The standard definition for the inverse of a matrix fails if the matrix is not square or singular. However, one can generalize the inverse using singular value decomposition. Any rectangular real matrix <span class="math inline">\(\mathbf{M}\)</span> can be decomposed as <span class="math inline">\(\mathbf{M=U \Sigma V^{&#39;}}\)</span>, where <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> are orthogonal and <span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix containing only the positive singular values. The pseudoinverse, also known as <strong>Moore-Penrose</strong> or generalized inverse is then obtained as</p>
<p><span class="math display">\[
\mathbf{M^+} = \mathbf{V \Sigma^{-1} U&#39;}
\]</span></p>
<p>Donât be confused due to notation: <span class="math inline">\(\Sigma\)</span> is not the covariance matrix here</p>
<p>With using the method of generalized inverse by <code>ppcor</code> and <code>corpcor</code><a href="https://pubmed.ncbi.nlm.nih.gov/16646851/">here</a> <span class="citation">(<a href="#ref-Schafer_2005" role="doc-biblioref">SchÃ¤fer and Strimmer 2005</a>)</span>:</p>
<div class="sourceCode" id="cb1006"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1006-1"><a href="regularized-covariance-matrix.html#cb1006-1" aria-hidden="true" tabindex="-1"></a><span class="co">#https://rdrr.io/cran/corpcor/man/pseudoinverse.html</span></span>
<span id="cb1006-2"><a href="regularized-covariance-matrix.html#cb1006-2" aria-hidden="true" tabindex="-1"></a>Si <span class="ot">&lt;-</span> corpcor<span class="sc">::</span><span class="fu">pseudoinverse</span>(S)</span>
<span id="cb1006-3"><a href="regularized-covariance-matrix.html#cb1006-3" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span>Si[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span>(<span class="fu">sqrt</span>(Si[<span class="dv">1</span>,<span class="dv">1</span>])<span class="sc">*</span><span class="fu">sqrt</span>(Si[<span class="dv">2</span>,<span class="dv">2</span>])) </span></code></pre></div>
<pre><code>## [1] -0.4823509</code></pre>
<div class="sourceCode" id="cb1008"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1008-1"><a href="regularized-covariance-matrix.html#cb1008-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ppcor</span></span>
<span id="cb1008-2"><a href="regularized-covariance-matrix.html#cb1008-2" aria-hidden="true" tabindex="-1"></a>pc <span class="ot">&lt;-</span> ppcor<span class="sc">::</span><span class="fu">pcor</span>(X)</span></code></pre></div>
<pre><code>## Warning in ppcor::pcor(X): The inverse of variance-covariance matrix is
## calculated using Moore-Penrose generalized matrix invers due to its determinant
## of zero.</code></pre>
<pre><code>## Warning in sqrt((n - 2 - gp)/(1 - pcor^2)): NaNs produced</code></pre>
<div class="sourceCode" id="cb1011"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1011-1"><a href="regularized-covariance-matrix.html#cb1011-1" aria-hidden="true" tabindex="-1"></a>pc<span class="sc">$</span>estimate</span></code></pre></div>
<pre><code>##              [,1]        [,2]        [,3]       [,4]        [,5]          [,6]
##  [1,]  1.00000000 -0.48235089 -0.43471080 -0.6132218  0.59239395 -0.1515785108
##  [2,] -0.48235089  1.00000000 -0.85835176 -0.7984656  0.08341783  0.1922476120
##  [3,] -0.43471080 -0.85835176  1.00000000 -0.8107355 -0.06073205 -0.1395456329
##  [4,] -0.61322177 -0.79846556 -0.81073546  1.0000000  0.11814582 -0.3271223659
##  [5,]  0.59239395  0.08341783 -0.06073205  0.1181458  1.00000000 -0.4056046405
##  [6,] -0.15157851  0.19224761 -0.13954563 -0.3271224 -0.40560464  1.0000000000
##  [7,]  0.81227748  0.76456650  0.76563183  0.7861380 -0.07927500  0.2753626258
##  [8,] -0.74807903 -0.67387820 -0.64812735 -0.6321303 -0.04063566 -0.2660628754
##  [9,]  0.79435763  0.32542381  0.52481792  0.5106454 -0.08284875  0.5458020595
## [10,]  0.01484899 -0.34289348  0.01425498 -0.2181704 -0.41275254  0.0006582396
##             [,7]        [,8]        [,9]         [,10]
##  [1,]  0.8122775 -0.74807903  0.79435763  0.0148489929
##  [2,]  0.7645665 -0.67387820  0.32542381 -0.3428934821
##  [3,]  0.7656318 -0.64812735  0.52481792  0.0142549759
##  [4,]  0.7861380 -0.63213032  0.51064540 -0.2181703890
##  [5,] -0.0792750 -0.04063566 -0.08284875 -0.4127525424
##  [6,]  0.2753626 -0.26606288  0.54580206  0.0006582396
##  [7,]  1.0000000  0.96888026 -0.84167300  0.2703213517
##  [8,]  0.9688803  1.00000000  0.84455999 -0.3746342510
##  [9,] -0.8416730  0.84455999  1.00000000 -0.0701428715
## [10,]  0.2703214 -0.37463425 -0.07014287  1.0000000000</code></pre>
<div class="sourceCode" id="cb1013"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1013-1"><a href="regularized-covariance-matrix.html#cb1013-1" aria-hidden="true" tabindex="-1"></a><span class="co"># corpcor with pseudo inverse</span></span>
<span id="cb1013-2"><a href="regularized-covariance-matrix.html#cb1013-2" aria-hidden="true" tabindex="-1"></a>corpcor<span class="sc">::</span><span class="fu">cor2pcor</span>(S)</span></code></pre></div>
<pre><code>##              [,1]        [,2]        [,3]       [,4]        [,5]          [,6]
##  [1,]  1.00000000 -0.48235089 -0.43471080 -0.6132218  0.59239395 -0.1515785108
##  [2,] -0.48235089  1.00000000 -0.85835176 -0.7984656  0.08341783  0.1922476120
##  [3,] -0.43471080 -0.85835176  1.00000000 -0.8107355 -0.06073205 -0.1395456329
##  [4,] -0.61322177 -0.79846556 -0.81073546  1.0000000  0.11814582 -0.3271223659
##  [5,]  0.59239395  0.08341783 -0.06073205  0.1181458  1.00000000 -0.4056046405
##  [6,] -0.15157851  0.19224761 -0.13954563 -0.3271224 -0.40560464  1.0000000000
##  [7,]  0.81227748  0.76456650  0.76563183  0.7861380 -0.07927500  0.2753626258
##  [8,] -0.74807903 -0.67387820 -0.64812735 -0.6321303 -0.04063566 -0.2660628754
##  [9,]  0.79435763  0.32542381  0.52481792  0.5106454 -0.08284875  0.5458020595
## [10,]  0.01484899 -0.34289348  0.01425498 -0.2181704 -0.41275254  0.0006582396
##             [,7]        [,8]        [,9]         [,10]
##  [1,]  0.8122775 -0.74807903  0.79435763  0.0148489929
##  [2,]  0.7645665 -0.67387820  0.32542381 -0.3428934821
##  [3,]  0.7656318 -0.64812735  0.52481792  0.0142549759
##  [4,]  0.7861380 -0.63213032  0.51064540 -0.2181703890
##  [5,] -0.0792750 -0.04063566 -0.08284875 -0.4127525424
##  [6,]  0.2753626 -0.26606288  0.54580206  0.0006582396
##  [7,]  1.0000000  0.96888026 -0.84167300  0.2703213517
##  [8,]  0.9688803  1.00000000  0.84455999 -0.3746342510
##  [9,] -0.8416730  0.84455999  1.00000000 -0.0701428715
## [10,]  0.2703214 -0.37463425 -0.07014287  1.0000000000</code></pre>
</div>
<div id="ridge-ell_2-and-glasso-ell_1" class="section level2 hasAnchor" number="35.3">
<h2><span class="header-section-number">35.3</span> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)<a href="regularized-covariance-matrix.html#ridge-ell_2-and-glasso-ell_1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The effect of the ridge penalty is also studied from the perspective of singular values.</p>
<p>When <span class="math inline">\(\mathbf{X}\)</span> is high-dimensional the regression parameter <span class="math inline">\(\beta\)</span> cannot be estimated. This is only the practical consequence of high-dimensionality: the expression <span class="math inline">\(\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \mathbf{Y}\)</span> cannot be evaluated numerically. But the problem arising from the high-dimensionality of the data is more fundamental. To appreciate this, consider the normal equations: <span class="math inline">\(\mathbf{X}^{\top} \mathbf{X} \boldsymbol{\beta}=\mathbf{X}^{\top} \mathbf{Y}\)</span>. The matrix <span class="math inline">\(\mathbf{X}^{\top} \mathbf{X}\)</span> is of rank <span class="math inline">\(n\)</span>, while <span class="math inline">\(\boldsymbol{\beta}\)</span> is a vector of length <span class="math inline">\(p\)</span>. Hence, while there are <span class="math inline">\(p\)</span> unknowns, the system of linear equations from which these are to be solved effectively comprises <span class="math inline">\(n\)</span> degrees of freedom. If <span class="math inline">\(p&gt;n\)</span>, the vector <span class="math inline">\(\boldsymbol{\beta}\)</span> cannot uniquely be determined from this system of equations.</p>
<p>We can express the effect of the ridge penalty from the perspective of singular values.</p>
<p>In case of singular <span class="math inline">\(\mathbf{X}^{T} \mathbf{X}\)</span> its inverse <span class="math inline">\(\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1}\)</span> is not defined. Consequently, the OLS estimator
<span class="math display">\[
\hat{\boldsymbol{\beta}}=\left(\mathbf{X}^{\top} \mathbf{X}\right)^{-1} \mathbf{X}^{\top} \mathbf{Y}
\]</span>
does not exist. This happens in high-dimensional data. An ad-hoc solution adds <span class="math inline">\(\lambda \mathbf{I}\)</span> to <span class="math inline">\(\mathbf{X}^{T} \mathbf{X}\)</span>, leading to:
<span class="math display">\[
\hat{\boldsymbol{\beta}}(\lambda)=\left(\mathbf{X}^{\top} \mathbf{X}+\lambda \mathbf{I}_{p p}\right)^{-1} \mathbf{X}^{\top} \mathbf{Y}
\]</span></p>
<p>This is called the ridge estimator.</p>
<p>Let the columns of <span class="math inline">\(X\)</span> be standardized, as well as <span class="math inline">\(y\)</span> itself. (This means we no longer need a constant column in <span class="math inline">\(X\)</span>). The ad-hoc ridge estimator minimizes the loss function:</p>
<p><span class="math display">\[
\mathcal{L}(\boldsymbol{\beta} ; \lambda)=\|\mathbf{Y}-\mathbf{X} \boldsymbol{\beta}\|_{2}^{2}+\lambda\|\boldsymbol{\beta}\|_{2}^{2}
\]</span>
Or constrained optimization problem</p>
<p><span class="math display">\[
\arg \min _{\beta}\|\mathbf{y}-\mathbf{X} \beta\|^{2}+\lambda\|\beta\|^{2} \quad \lambda&gt;0
\]</span></p>
<p>Take the derivative of the loss function:
<span class="math display">\[
\frac{\partial}{\partial \boldsymbol{\beta}} \mathcal{L}(\boldsymbol{\beta} ; \lambda)=-2 \mathbf{X}^{\top} \mathbf{y}+2\left(\mathbf{X}^{\top} \mathbf{X}+\lambda \mathbf{I}_{p}\right) \boldsymbol{\beta}
\]</span>
Hence:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\beta}_{R} &amp;=\left(X^{\prime} X+\lambda I_{p}\right)^{-1} X^{\prime} y \\
&amp;=\left(V \Sigma^{2} V^{\prime}+\lambda I_{p}\right)^{-1} V \Sigma U^{\prime} y \\
&amp;=\left(V \Sigma^{2} V^{\prime}+\lambda V V^{\prime}\right)^{-1} V \Sigma U^{\prime} y \\
&amp;=\left(V\left(\Sigma^{2}+\lambda I_p\right) V^{\prime}\right)^{-1} V \Sigma U^{\prime} y \\
&amp;=V\left(\Sigma^{2}+\lambda I_p\right)^{-1} V^{\prime} V \Sigma U^{\prime} y \\
&amp;=V\left(\Sigma^{2}+\lambda I_p\right)^{-1} \Sigma U^{\prime} y .
\end{aligned}
\]</span></p>
<p>The columns of <span class="math inline">\(\mathbf{U}_{x}\)</span> and <span class="math inline">\(\mathbf{V}_{x}\)</span> are orthogonal: <span class="math inline">\(\mathbf{U}_{x}^{\top} \mathbf{U}_{x}=\mathbf{I}_{n n}=\mathbf{U}_{x} \mathbf{U}_{x}^{\top}\)</span> and <span class="math inline">\(\mathbf{V}_{x}^{\top} \mathbf{V}_{x}=\mathbf{I}_{p p}=\mathbf{V}_{x} \mathbf{V}_{x}^{\top}\)</span>. The difference with OLS?</p>
<p><span class="math display">\[
V\Sigma^{-1}U&#39;y =  \beta_{OLS}\\
V\Sigma^{-2}\Sigma U&#39;y =  \beta_{OLS}
\]</span></p>
<p>The difference between this and <span class="math inline">\(\beta_{OLS}\)</span> and <span class="math inline">\(\beta_{R}\)</span> is the replacement of <span class="math inline">\(\Sigma^{-1}=\Sigma^{-2} \Sigma\)</span> by <span class="math inline">\(\left(\Sigma^{2}+\lambda I_p\right)^{-1} \Sigma\)</span>. In effect, this multiplies the original by the fraction <span class="math inline">\(\Sigma^{2} /\left(\Sigma^{2}+\lambda\right) .\)</span> Because (when <span class="math inline">\(\left.\lambda&gt;0\right)\)</span> the denominator is obviously greater than the numerator, the parameter estimates <strong>shrink towards zero,</strong> i.e., write <span class="math inline">\((\mathbf{\Sigma})_{j j}=d_{j j}\)</span> to obtain <span class="math inline">\(d_{i i}/\left(d_{i i}^{2}+\lambda\right)\)</span>. So that</p>
<p><span class="math display">\[
\frac{d_{j j}^{-1}}{\text { OLS }} \geq \frac{d_{j j} /\left(d_{j j}^{2}+\lambda\right)}{\text { ridge }}
\]</span></p>
<p>As such, the rotated coefficients must shrink, but it is possible, when <span class="math inline">\(\lambda\)</span> is sufficiently small, for some of the <span class="math inline">\(\hat{\beta}_{R}\)</span> themselves actually to increase in size.</p>
<p>Interest in graphical models that combine a probabilistic description (through a multivariate distribution) of a system with a graph that depicts the systemâs structure (capturing dependence relationships), has surged in recent years<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a>. In its trail this has renewed the attention to the estimation of precision matrices as they harbor the conditional (in)dependencies among jointly distributed variates. In particular, with the advent of high-dimensional data, for which traditional precision estimators are not well-defined, this brought about several novel precision estimators.</p>
<p>Generally, these novel estimators overcome the undersampling by maximization of the log-likelihood augmented with a so-called penalty. A penalty discourages large (in some sense) values among the elements of the precision matrix estimate. This reduces the risk of overfitting but also yields a well-defined penalized precision matrix estimator.</p>
<p>Datasets where <span class="math inline">\(p&gt;n\)</span> are starting to be common, so what now?</p>
<p>To solve the problem, penalized estimators, like <code>rags2ridges</code> (see, <a href="https://cran.r-project.org/web/packages/rags2ridges/vignettes/rags2ridges.html">Introduction to rags2ridges</a>), adds a so-called ridge penalty to the likelihood above (this method is also called <span class="math inline">\(\ell_{2}\)</span> shrinkage and works by âshrinkingâ the eigenvalues of <span class="math inline">\(S\)</span> in a particular manner to combat that they âexplodeâ when <span class="math inline">\(p \geq n\)</span>. âShrinkingâ is a âbiased estimationâ as a means of variance reduction of S.</p>
<p>Their algorithm solves the following:</p>
<p><span class="math display">\[
\ell(\Omega ; S)=\ln |\Omega|-\operatorname{tr}(S \Omega)-\frac{\lambda}{2}\|\Omega-T\|_{2}^{2}
\]</span></p>
<p>where <span class="math inline">\(\lambda&gt;0\)</span> is the ridge penalty parameter, <span class="math inline">\(T\)</span> is a <span class="math inline">\(p \times p\)</span> known target matrix and <span class="math inline">\(\|\cdot\|_{2}\)</span> is the <span class="math inline">\(\ell_{2}\)</span>-norm. Assume for now the target matrix is an all zero matrix and thus out of the equation. The core function of <code>rags2ridges</code> is <code>ridgeP</code> which computes this estimate in a fast manner.</p>
<p>The ridge precision estimation can be summarized with the following steps <a href="http://www.few.vu.nl/~wvanwie/presentations/WNvanWieringen_RidgeEstimationOfGGM.pdf">See</a>:</p>
<p>The ridge penalty:
<span class="math display">\[
\frac{1}{2} \lambda_{2}\left\|\boldsymbol{\Sigma}^{-1}\right\|_{2}^{2}
\]</span></p>
<p>When writing <span class="math inline">\(\Omega=\Sigma^{-1}\)</span> the ridge penalty is:
<span class="math display">\[
\|\boldsymbol{\Omega}\|_{2}^{2}=\sum_{j_{1}, j_{2}=1}^{p}\left[(\boldsymbol{\Omega})_{j_{1}, j_{2}}\right]^{2}
\]</span>
For a 2x2 precision matrix this penalized estimation problems can be viewed as constrained optimization problem:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;{\left[\Omega_{11}\right]^{2}+2\left[\Omega_{12}\right]^{2}} +\left[\Omega_{22}\right]^{2} \leq c\left(\lambda_{2}\right)
\end{aligned}
\]</span>
Consider the ridge loss function:</p>
<p><span class="math display">\[
\log (|\boldsymbol{\Omega}|)-\operatorname{tr}(\mathbf{S} \boldsymbol{\Omega})-\frac{1}{2} \lambda_{2} \operatorname{tr}\left(\boldsymbol{\Omega} \boldsymbol{\Omega}^{\mathrm{T}}\right)
\]</span></p>
<p>Equation of the derivative w.r.t. the precision matrix to zero yields the estimating equation:</p>
<p><span class="math display">\[
\boldsymbol{\Omega}^{-1}-\mathbf{S}-\lambda_{2} \boldsymbol{\Omega}=\mathbf{0}_{p \times p}
\]</span>
Matrix algebra then yields:</p>
<p><span class="math display">\[
\widehat{\boldsymbol{\Omega}}\left(\lambda_{2}\right)=\left[\frac{1}{2} \mathbf{S}+\left(\lambda_{2} \mathbf{I}_{p \times p}+\frac{1}{4} \mathbf{S}^{2}\right)^{1 / 2}\right]^{-1}
\]</span>
Thus,</p>
<p><span class="math display">\[
\widehat{\boldsymbol{\Sigma}}\left(\lambda_{2}\right)=\frac{1}{2} \mathbf{S}+\left(\lambda_{2} \mathbf{I}_{p \times p}+\frac{1}{4} \mathbf{S}^{2}\right)^{1 / 2}
\]</span>
The derived ridge covariance estimator is positive definite, ie itâs symmetric and all its eigenvalues are positive. (Remember, when the matrix is symmetric, its trace is the sum of eigenvalues. Since the diagonal entries are all positive - variances - the trace of this covariance matrix is positive - <a href="https://www.robots.ox.ac.uk/~davidc/pubs/tt2015_dac1.pdf">see</a>)</p>
<p>For <span class="math inline">\(\lambda_{2}=0\)</span>, we obtain:</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{\Sigma}}(0) &amp;=\frac{1}{2} \mathbf{S}+\left(\frac{1}{4} \mathbf{S}^{2}\right)^{1 / 2} \\
&amp;=\frac{1}{2} \mathbf{S}+\frac{1}{2} \mathbf{S}=\mathbf{S}
\end{aligned}
\]</span></p>
<p>For large enough <span class="math inline">\(\lambda_{2}\)</span>:</p>
<p><span class="math display">\[
\widehat{\boldsymbol{\Sigma}}\left(\lambda_{2}\right) \approx \lambda_{2} \mathbf{I}_{p \times p}
\]</span>
The penalty parameter <span class="math inline">\(\lambda\)</span> shrinks the values of <span class="math inline">\(P\)</span> such toward 0 (when <span class="math inline">\(T=0\)</span> ), i.e.Â very larges values of <span class="math inline">\(\lambda\)</span> makes <span class="math inline">\(P\)</span> âsmallâ and more stable whereas smaller values of <span class="math inline">\(\lambda\)</span> makes the <span class="math inline">\(P\)</span> tend toward the (possibly nonexistent) <span class="math inline">\(S^{-1}\)</span>.</p>
<p>Letâs try some simulations:</p>
<div class="sourceCode" id="cb1015"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1015-1"><a href="regularized-covariance-matrix.html#cb1015-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We did this before</span></span>
<span id="cb1015-2"><a href="regularized-covariance-matrix.html#cb1015-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">6</span></span>
<span id="cb1015-3"><a href="regularized-covariance-matrix.html#cb1015-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb1015-4"><a href="regularized-covariance-matrix.html#cb1015-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb1015-5"><a href="regularized-covariance-matrix.html#cb1015-5" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span> (<span class="fu">rnorm</span>(n<span class="sc">*</span>p), n, p)</span>
<span id="cb1015-6"><a href="regularized-covariance-matrix.html#cb1015-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1015-7"><a href="regularized-covariance-matrix.html#cb1015-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Cov. &amp; Precision Matrices</span></span>
<span id="cb1015-8"><a href="regularized-covariance-matrix.html#cb1015-8" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">cov</span>(X)</span>
<span id="cb1015-9"><a href="regularized-covariance-matrix.html#cb1015-9" aria-hidden="true" tabindex="-1"></a>S</span></code></pre></div>
<pre><code>##               [,1]        [,2]        [,3]        [,4]         [,5]        [,6]
##  [1,]  0.889211221 -0.17223814 -0.36660043  0.35320957 -0.629545741 -0.27978848
##  [2,] -0.172238139  0.34416306 -0.09280183 -0.04282613  0.139236591 -0.26060435
##  [3,] -0.366600426 -0.09280183  1.46701338 -0.50796342 -0.024550727 -0.11504405
##  [4,]  0.353209573 -0.04282613 -0.50796342  1.24117592 -0.292005017  0.42646139
##  [5,] -0.629545741  0.13923659 -0.02455073 -0.29200502  0.553562287  0.26275658
##  [6,] -0.279788479 -0.26060435 -0.11504405  0.42646139  0.262756584  0.81429052
##  [7,]  0.143364328 -0.14895377  0.29598156  0.30839120 -0.275296303  0.04418159
##  [8,] -0.273835576  0.17201439 -0.31052657 -0.39667581  0.376175973 -0.02536104
##  [9,] -0.008919669  0.24390178 -0.50198614  0.52741301  0.008044799 -0.01297542
## [10,] -0.304722895  0.33936685 -1.08854590  0.20441696  0.499437080  0.20218868
##              [,7]        [,8]         [,9]      [,10]
##  [1,]  0.14336433 -0.27383558 -0.008919669 -0.3047229
##  [2,] -0.14895377  0.17201439  0.243901782  0.3393668
##  [3,]  0.29598156 -0.31052657 -0.501986137 -1.0885459
##  [4,]  0.30839120 -0.39667581  0.527413006  0.2044170
##  [5,] -0.27529630  0.37617597  0.008044799  0.4994371
##  [6,]  0.04418159 -0.02536104 -0.012975416  0.2021887
##  [7,]  0.37576405 -0.40476558  0.046294293 -0.4691147
##  [8,] -0.40476558  0.46612332 -0.026813818  0.5588965
##  [9,]  0.04629429 -0.02681382  0.540956259  0.5036908
## [10,] -0.46911465  0.55889647  0.503690786  1.3107637</code></pre>
<div class="sourceCode" id="cb1017"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1017-1"><a href="regularized-covariance-matrix.html#cb1017-1" aria-hidden="true" tabindex="-1"></a><span class="fu">try</span>(<span class="fu">solve</span>(S), <span class="at">silent =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## Error in solve.default(S) : 
##   system is computationally singular: reciprocal condition number = 3.99819e-19</code></pre>
<div class="sourceCode" id="cb1019"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1019-1"><a href="regularized-covariance-matrix.html#cb1019-1" aria-hidden="true" tabindex="-1"></a><span class="co"># With Ridge, lambda = 0</span></span>
<span id="cb1019-2"><a href="regularized-covariance-matrix.html#cb1019-2" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb1019-3"><a href="regularized-covariance-matrix.html#cb1019-3" aria-hidden="true" tabindex="-1"></a>SRidge <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>S <span class="sc">+</span> expm<span class="sc">::</span><span class="fu">sqrtm</span>(lambda<span class="sc">*</span><span class="fu">diag</span>(<span class="dv">1</span>, p)<span class="sc">+</span><span class="fl">0.25</span><span class="sc">*</span>(S<span class="sc">%*%</span>S))</span>
<span id="cb1019-4"><a href="regularized-covariance-matrix.html#cb1019-4" aria-hidden="true" tabindex="-1"></a>SRidge</span></code></pre></div>
<pre><code>##                  [,1]           [,2]           [,3]           [,4]
##  [1,]  0.889211226+0i -0.17223814+0i -0.36660042+0i  0.35320957+0i
##  [2,] -0.172238138+0i  0.34416307+0i -0.09280183-0i -0.04282613+0i
##  [3,] -0.366600424+0i -0.09280183-0i  1.46701339+0i -0.50796342-0i
##  [4,]  0.353209571+0i -0.04282613+0i -0.50796342-0i  1.24117592+0i
##  [5,] -0.629545737+0i  0.13923659+0i -0.02455073-0i -0.29200502+0i
##  [6,] -0.279788477-0i -0.26060434-0i -0.11504405+0i  0.42646139-0i
##  [7,]  0.143364330+0i -0.14895377+0i  0.29598156+0i  0.30839120+0i
##  [8,] -0.273835576+0i  0.17201439+0i -0.31052657+0i -0.39667581+0i
##  [9,] -0.008919667-0i  0.24390178-0i -0.50198614+0i  0.52741301-0i
## [10,] -0.304722894+0i  0.33936685+0i -1.08854590-0i  0.20441696+0i
##                       [,5]           [,6]           [,7]           [,8]
##  [1,] -6.295457e-01+0e+00i -0.27978848+0i  0.14336433+0i -0.27383558+0i
##  [2,]  1.392366e-01+0e+00i -0.26060434-0i -0.14895377+0i  0.17201439+0i
##  [3,] -2.455073e-02-0e+00i -0.11504405+0i  0.29598156+0i -0.31052657+0i
##  [4,] -2.920050e-01+0e+00i  0.42646139-0i  0.30839120+0i -0.39667581+0i
##  [5,]  5.535623e-01+0e+00i  0.26275658-0i -0.27529630+0i  0.37617597-0i
##  [6,]  2.627566e-01-0e+00i  0.81429053+0i  0.04418159-0i -0.02536104+0i
##  [7,] -2.752963e-01+0e+00i  0.04418159-0i  0.37576405+0i -0.40476558+0i
##  [8,]  3.761760e-01-0e+00i -0.02536104+0i -0.40476558+0i  0.46612332+0i
##  [9,]  8.044802e-03-1e-09i -0.01297542+0i  0.04629429-0i -0.02681382-0i
## [10,]  4.994371e-01+0e+00i  0.20218868-0i -0.46911465+0i  0.55889646+0i
##                       [,9]         [,10]
##  [1,] -8.919667e-03+0e+00i -0.3047229+0i
##  [2,]  2.439018e-01-0e+00i  0.3393668+0i
##  [3,] -5.019861e-01+0e+00i -1.0885459-0i
##  [4,]  5.274130e-01-0e+00i  0.2044170+0i
##  [5,]  8.044802e-03-1e-09i  0.4994371+0i
##  [6,] -1.297542e-02+0e+00i  0.2021887-0i
##  [7,]  4.629429e-02-0e+00i -0.4691147+0i
##  [8,] -2.681382e-02-0e+00i  0.5588965+0i
##  [9,]  5.409563e-01+0e+00i  0.5036908-0i
## [10,]  5.036908e-01-0e+00i  1.3107637+0i</code></pre>
<div class="sourceCode" id="cb1021"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1021-1"><a href="regularized-covariance-matrix.html#cb1021-1" aria-hidden="true" tabindex="-1"></a><span class="co"># With Ridge, lambda = 0</span></span>
<span id="cb1021-2"><a href="regularized-covariance-matrix.html#cb1021-2" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">=</span> <span class="fl">0.5</span></span>
<span id="cb1021-3"><a href="regularized-covariance-matrix.html#cb1021-3" aria-hidden="true" tabindex="-1"></a>SRidge <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>S <span class="sc">+</span> expm<span class="sc">::</span><span class="fu">sqrtm</span>(lambda<span class="sc">*</span><span class="fu">diag</span>(<span class="dv">1</span>, p)<span class="sc">+</span><span class="fl">0.25</span><span class="sc">*</span>(S<span class="sc">%*%</span>S))</span>
<span id="cb1021-4"><a href="regularized-covariance-matrix.html#cb1021-4" aria-hidden="true" tabindex="-1"></a>SRidge</span></code></pre></div>
<pre><code>##              [,1]        [,2]        [,3]        [,4]        [,5]        [,6]
##  [1,]  1.37069495 -0.13010676 -0.26194680  0.29808331 -0.48369828 -0.20441616
##  [2,] -0.13010676  0.94030145 -0.10511356 -0.04366041  0.11696761 -0.16508806
##  [3,] -0.26194680 -0.10511356  1.88019154 -0.44099096 -0.04204807 -0.10402368
##  [4,]  0.29808331 -0.04366041 -0.44099096  1.65839433 -0.23659110  0.31883220
##  [5,] -0.48369828  0.11696761 -0.04204807 -0.23659110  1.13995922  0.19056877
##  [6,] -0.20441616 -0.16508806 -0.10402368  0.31883220  0.19056877  1.27387916
##  [7,]  0.13649132 -0.12065058  0.24458809  0.23574080 -0.22740740  0.02961578
##  [8,] -0.23352284  0.14255435 -0.25606790 -0.30118441  0.30653069 -0.01411260
##  [9,]  0.01451734  0.16994890 -0.43236453  0.41233338  0.01096246  0.01961695
## [10,] -0.24477072  0.27432427 -0.90767449  0.17451629  0.40902909  0.17254457
##              [,7]          [,8]          [,9]      [,10]
##  [1,]  0.13649132 -0.2335228409  0.0145173438 -0.2447707
##  [2,] -0.12065058  0.1425543547  0.1699489031  0.2743243
##  [3,]  0.24458809 -0.2560679036 -0.4323645280 -0.9076745
##  [4,]  0.23574080 -0.3011844066  0.4123333844  0.1745163
##  [5,] -0.22740740  0.3065306880  0.0109624604  0.4090291
##  [6,]  0.02961578 -0.0141126044  0.0196169539  0.1725446
##  [7,]  0.98901314 -0.3131878342  0.0113671901 -0.3860672
##  [8,] -0.31318783  1.0735641056 -0.0002152917  0.4596644
##  [9,]  0.01136719 -0.0002152917  1.1067644836  0.4153144
## [10,] -0.38606724  0.4596643602  0.4153143722  1.7902418</code></pre>
<div class="sourceCode" id="cb1023"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1023-1"><a href="regularized-covariance-matrix.html#cb1023-1" aria-hidden="true" tabindex="-1"></a><span class="fu">solve</span>(SRidge)</span></code></pre></div>
<pre><code>##              [,1]        [,2]        [,3]        [,4]         [,5]        [,6]
##  [1,]  0.96296745  0.08426275  0.20930726 -0.11025252  0.291694924  0.15074465
##  [2,]  0.08426275  1.19227678 -0.02462345 -0.00166857 -0.044537957  0.19103259
##  [3,]  0.20930726 -0.02462345  0.82635632  0.13394492 -0.034994680  0.02204074
##  [4,] -0.11025252 -0.00166857  0.13394492  0.83443682  0.110827841 -0.21525839
##  [5,]  0.29169492 -0.04453796 -0.03499468  0.11082784  1.172793866 -0.14437564
##  [6,]  0.15074465  0.19103259  0.02204074 -0.21525839 -0.144375635  0.91917727
##  [7,] -0.01374603  0.05660639 -0.10278693 -0.14530081  0.095777812 -0.02913162
##  [8,]  0.08062547 -0.05892007  0.10891732  0.19098281 -0.139290570  0.02249687
##  [9,]  0.04687403 -0.14790576  0.13924322 -0.23015924  0.005835323  0.06518474
## [10,]  0.11990434 -0.13008516  0.36174283 -0.05980134 -0.180815973 -0.05928821
##              [,7]        [,8]         [,9]       [,10]
##  [1,] -0.01374603  0.08062547  0.046874026  0.11990434
##  [2,]  0.05660639 -0.05892007 -0.147905757 -0.13008516
##  [3,] -0.10278693  0.10891732  0.139243218  0.36174283
##  [4,] -0.14530081  0.19098281 -0.230159243 -0.05980134
##  [5,]  0.09577781 -0.13929057  0.005835323 -0.18081597
##  [6,] -0.02913162  0.02249687  0.065184740 -0.05928821
##  [7,]  1.22649819  0.18315549 -0.069854206  0.16609483
##  [8,]  0.18315549  1.21488158  0.053197052 -0.19846421
##  [9,] -0.06985421  0.05319705  1.131616449 -0.17675283
## [10,]  0.16609483 -0.19846421 -0.176752827  0.95895605</code></pre>
<p>There are many ways to regularize covariance estimation. Some of these âad-hocâ estimators are often referred to as âridgeâ estimates:</p>
<p><span class="math display">\[
\mathbf{S}+\lambda_{a} \mathbf{I}_{p \times p} \quad \text { for } \quad \lambda_{a}&gt;0
\]</span>
and:
<span class="math display">\[
\left(1-\lambda_{a}\right) \mathbf{S}+\lambda_{a} \mathbf{T} \quad \text { for } \quad \lambda_{a} \in(0,1)
\]</span></p>
<p>where <span class="math inline">\(\mathrm{T}\)</span> is some nonrandom, positive definite matrix. Both are not derived from a penalized loss function, but are simply ad-hoc fixes to resolve the singularity of the estimate. To evaluate ad-hoc and Ridge estimators, we compare the eigenvalues of the ad-hoc and ridge estimator of the covariance matrix.</p>
<p>Consider the eigen-decomposition: <span class="math inline">\(\mathbf{S}=\mathbf{V D V}^{\mathrm{T}}\)</span> with <span class="math inline">\(\mathbf{V}\)</span> and <span class="math inline">\(\mathrm{D}\)</span> the eigenvalue and -vector matrices. The eigenvalues of <span class="math inline">\(\widehat{\mathbf{\Sigma}}\left(\lambda_{2}\right)\)</span> are then:</p>
<p><span class="math display">\[
d_{j}\left[\widehat{\boldsymbol{\Sigma}}\left(\lambda_{2}\right)\right]=\frac{1}{2} d_{j}+\left(\lambda_{2}+\frac{1}{4} d_{j}^{2}\right)^{1 / 2}
\]</span></p>
<p>Writing <span class="math inline">\(d_{j}=(\mathbf{D})_{j j}\)</span> it is easily seen that:</p>
<p><span class="math display">\[
\lambda_{a}+d_{j} \geq \frac{1}{2} d_{j}+\sqrt{\lambda_{a}^{2}+\frac{1}{4} d_{j}^{2}}
\]</span></p>
<p>Thus, <strong>the ad-hoc estimator shrinks the eigenvalues of the sample covariance matrix more than the ridge estimator</strong>.</p>
<p>Why a target <span class="math inline">\(T\)</span>?</p>
<ol style="list-style-type: decimal">
<li>Both the ad-hoc and ridge covariance estimator converge to:
<span class="math display">\[
\widehat{\boldsymbol{\Sigma}}\left(\lambda_{2}\right) \approx \lambda_{2} \mathbf{I}_{p \times p} \quad \text {for large enough} \quad\lambda_{2}
\]</span>
Its inverse (the precision matrix) converges to the zero matrix including the diagonal elements! Consequently, the partial correlation of this matrix are undefined.<br />
</li>
<li>If signal-to-noise ratio is poor, why not provide a hint.</li>
</ol>
<p>The target matrix <span class="math inline">\(T\)</span> is a matrix the same size as <span class="math inline">\(P\)</span> which the estimate is âshrunkenâ toward, i.e.Â for large values of <span class="math inline">\(\lambda\)</span> the estimate goes toward <span class="math inline">\(T\)</span>. The choice of the target is another subject. While one might first think that the all-zeros <span class="math inline">\(T=[0]\)</span> would be a default it is intuitively not a good target. This is because weâd like an estimate that is positive definite (the matrix-equivalent to at positive number) and the null-matrix is not positive definite.</p>
<p>If one has a very good prior estimate or some other information this might used to construct the target. In the absence of such knowledge, the default could be a data-driven diagonal matrix. The function <code>default.target()</code> offers some different approaches to selecting this. A good choice here is often the diagonal matrix times the reciprocal mean of the eigenvalues of the sample covariance as entries. See <code>?default.target</code> for more choices.</p>
<p>To ensure the ridge precision estimate converges to a positive definite target matrix <span class="math inline">\(\mathbf{T}\)</span> , the latter is incorporated in the penalty:</p>
<p><span class="math display">\[
\frac{1}{2} \lambda_{2} \operatorname{tr}\left[(\boldsymbol{\Omega}-\mathbf{T})(\boldsymbol{\Omega}-\mathbf{T})^{\mathrm{T}}\right]
\]</span></p>
<p>Clearly, the penalty is minimized for <span class="math inline">\(\Omega=\mathbf{T}\)</span>. One expects that, for large <span class="math inline">\(\lambda_{2}\)</span>, the maximization of the penalized log-likelihood requires the minimization of the penalty: the optimum moves close to <span class="math inline">\(\mathbf{T}\)</span>.</p>
<p>The log-likelihood augmented with this âtargetâ-penalty is maximized by:</p>
<p><span class="math display">\[
\left\{\frac{1}{2}\left(\mathbf{S}-\lambda_{2} \mathbf{T}\right)+\left[\lambda_{2} \mathbf{I}_{p \times p}+\frac{1}{4}\left(\mathbf{S}-\lambda_{2} \mathbf{T}\right)^{2}\right]^{1 / 2}\right\}^{-1}
\]</span>
For generalized ridge precision estimator one can show that:</p>
<p><span class="math display">\[
\lim _{\lambda_{2} \rightarrow \infty} \widehat{\boldsymbol{\Omega}}\left(\lambda_{2}\right)=\mathbf{T}
\]</span></p>
<p>and <span class="math inline">\(\widehat{\Omega}\left(\lambda_{2}\right) \succ 0\)</span> for all <span class="math inline">\(\lambda_{2}&gt;0\)</span></p>
<p>What Lambda should you choose? One strategy for choosing <span class="math inline">\(\lambda\)</span> is selecting it to be stable yet precise (a bias-variance trade-off). Automatic k-fold cross-validation can be done with <code>optPenalty.kCVauto()</code> is well suited for this.</p>
<p>LOOCV Simulation:</p>
<ul>
<li>Define a banded <span class="math inline">\(p \times p\)</span> precision matrix, <span class="math inline">\(p=100\)</span>.</li>
<li>Draw <span class="math inline">\(n=10\)</span> samples.</li>
<li>Determine optimal lambda by LOOCV.</li>
<li>Estimate precision matrix with and without target. Target is true precision.</li>
</ul>
<p><strong>Summary from their paper</strong></p>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0167947316301141">From the paper</a> <span class="citation">(<a href="#ref-Wessel_Ridge" role="doc-biblioref">Wieringen and Peeters 2019</a>)</span>, which defines a kind of sparsity method similar to L1 (glasso) but using L2:<br />
Let <span class="math inline">\(\mathbf{Y}_{i}, i=1, \ldots, n\)</span>, be a <span class="math inline">\(p\)</span>-dimensional random variate drawn from <span class="math inline">\(\mathcal{N}_{p}(\mathbf{0}, \mathbf{\Sigma})\)</span>. The maximum likelihood (ML) estimator of the precision matrix <span class="math inline">\(\boldsymbol{\Omega}=\boldsymbol{\Sigma}^{-1}\)</span> maximizes:</p>
<p><span class="math display">\[
\mathcal{L}(\boldsymbol{\Omega} ; \mathbf{S}) \propto \ln |\boldsymbol{\Omega}|-\operatorname{tr}(\mathbf{S} \boldsymbol{\Omega}) ~~~~~~~~~~~~~~~ (1)
\]</span></p>
<p>where <span class="math inline">\(\mathbf{S}\)</span> is the sample covariance estimate. If <span class="math inline">\(n&gt;p\)</span>, the log-likelihood achieves its maximum for <span class="math inline">\(\hat{\boldsymbol{\Omega}}^{\mathrm{ML}}=\mathbf{S}^{-1}\)</span>. In the high-dimensional setting where <span class="math inline">\(p&gt;n\)</span>, the sample covariance matrix is singular and its inverse is undefined. Consequently, so is <span class="math inline">\(\hat{\boldsymbol{\Omega}}^{\mathrm{ML}}\)</span>. A common workaround is the addition of a penalty to the <span class="math inline">\(\log\)</span>-likelihood (1). The <span class="math inline">\(\ell_{1}\)</span>-penalized estimation of the precision matrix was considered almost simultaneously. This (graphical) lasso estimate of <span class="math inline">\(\Omega\)</span> has attracted much attention due to the resulting sparse solution and has grown into an active area of research. Juxtaposed to situations in which sparsity is an asset are situations in which one is intrinsically interested in more accurate representations of the high-dimensional precision matrix. In addition, the true (graphical) model need not be (extremely) sparse in terms of containing many zero elements. In these cases we may prefer usage of a regularization method that shrinks the estimated elements of the precision matrix proportionally</p>
<p>Ridge estimators of the precision matrix currently in use can be roughly divided into two archetypes (cf.Â Ledoit and Wolf, 2004; SchÃ¤fer and Strimmer, 2005a). The first archetypal form of ridge estimator commonly is a convex combination of <span class="math inline">\(\mathbf{S}\)</span> and a positive definite (p.d.) target matrix <span class="math inline">\(\boldsymbol{\Gamma}: \hat{\mathbf{\Omega}}^{\mathrm{I}}\left(\lambda_{\mathrm{I}}\right)=\left[\left(1-\lambda_{\mathrm{I}}\right) \mathbf{S}+\lambda_{\mathrm{I}} \boldsymbol{\Gamma}\right]^{-1}\)</span>, with <span class="math inline">\(\lambda_{\mathrm{I}} \in(0,1]\)</span>. A common (low-dimensional) target choice is <span class="math inline">\(\Gamma\)</span> diagonal with <span class="math inline">\((\Gamma)_{j j}=(\mathbf{S})_{j j}\)</span> for <span class="math inline">\(j=1, \ldots, p .\)</span> This estimator has the desirable property of shrinking to <span class="math inline">\(\Gamma^{-}\)</span> when <span class="math inline">\(\lambda_{\mathrm{I}}=1\)</span> (maximum penalization). The estimator can be motivated from the bias-variance tradeoff as it seeks to balance the high-variance, low-bias matrix <span class="math inline">\(\mathbf{S}\)</span> with the lower-variance, higher-bias matrix <span class="math inline">\(\mathbf{\Gamma}\)</span>. It can also be viewed as resulting from the maximization of the following penalized log-likelihood:</p>
<p><span class="math display">\[
\ln |\boldsymbol{\Omega}|-\left(1-\lambda_{\mathrm{I}}\right) \operatorname{tr}(\mathbf{S} \boldsymbol{\Omega})-\lambda_{\mathrm{I}} \operatorname{tr}(\boldsymbol{\Omega} \boldsymbol{\Gamma})
\]</span></p>
<p>The penalized log-likelihood is obtained from the original log-likelihood (1) by the replacement of <span class="math inline">\(\mathbf{S}\)</span> by <span class="math inline">\(\left(1-\lambda_{\mathrm{I}}\right) \mathbf{S}\)</span> and the addition of a penalty. The estimate <span class="math inline">\(\hat{\boldsymbol{\Omega}}^{\mathrm{I}}\left(\lambda_{\mathrm{I}}\right)\)</span> can thus be viewed as a penalized ML estimate.</p>
</div>
<div id="whats-graphical---graphical-ridge-or-glasso" class="section level2 hasAnchor" number="35.4">
<h2><span class="header-section-number">35.4</span> Whatâs graphical - graphical ridge or glasso?<a href="regularized-covariance-matrix.html#whats-graphical---graphical-ridge-or-glasso" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A contemporary use for precision matrices is found in network reconstruction through graphical modeling (Network Analysis). Graphical modeling refers to a class of probabilistic models that uses graphs to express conditional (in)dependence relations between random variables.</p>
<p>In a multivariate normal model, <span class="math inline">\(p_{i j}=p_{j i}=0\)</span> if and only if <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(X_{j}\)</span> are conditionally independent when condition on all other variables. I.e. <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(X_{j}\)</span> are conditionally independent given all <span class="math inline">\(X_{k}\)</span> where <span class="math inline">\(k \neq i\)</span> and <span class="math inline">\(k \neq j\)</span> if and when the <span class="math inline">\(i j\)</span> th and <span class="math inline">\(j i\)</span> th elements of <span class="math inline">\(P\)</span> are zero. In real world applications, this means that <span class="math inline">\(P\)</span> is often relatively sparse (lots of zeros). This also points to the close relationship between <span class="math inline">\(P\)</span> and the partial correlations. <strong>The non-zero entries of the symmetric P matrix can be interpreted the edges of a graph where nodes correspond to the variables.</strong></p>
<p>The graphical lasso (<code>gLasso</code>) is the L1-equivalent to graphical ridge. A nice feature of the L1 penalty automatically induces sparsity and thus also select the edges in the underlying graph. The L2 penalty of <code>rags2ridges</code> relies on an extra step that selects the edges after <span class="math inline">\(P\)</span> is estimated. While some may argue this as a drawback (typically due to a lack of perceived simplicity), it is often beneficial to separate the âvariable selectionâ and estimation.</p>
<p>First, a separate post-hoc selection step allows for greater flexibility. Secondly, when co-linearity is present the L1 penalty is âunstableâ in the selection between the items. I.e. <strong>if 2 covariances are co-linear only one of them will typically be selected in a unpredictable way whereas the L2 will put equal weight on both and âaverageâ their effect</strong>. Ultimately, this means that the L2 estimate is typically more stable than the L1.</p>
<p>At last point to mention here is also that the true underlying graph might not always be very sparse (or sparse at all).</p>
<p>The <code>sparsify()</code> functions lets you select the non-zero entries of <span class="math inline">\(P\)</span> corresponding to edges. It supports a handful different approaches ranging from simple thresholding to false discovery rate based selection.</p>
<p>After edge selection, <code>GGMnetworkStats()</code> can be utilized to get summary statistics of the resulting graph topology.</p>
<p>Now, we will apply some packages on both glass and ridge. First LASSO:</p>
<div class="sourceCode" id="cb1025"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1025-1"><a href="regularized-covariance-matrix.html#cb1025-1" aria-hidden="true" tabindex="-1"></a><span class="co"># glasso</span></span>
<span id="cb1025-2"><a href="regularized-covariance-matrix.html#cb1025-2" aria-hidden="true" tabindex="-1"></a>gl <span class="ot">&lt;-</span> glasso<span class="sc">::</span><span class="fu">glasso</span>(S,<span class="at">rho=</span><span class="fl">0.2641</span>,<span class="at">approx=</span><span class="cn">FALSE</span>)[<span class="fu">c</span>(<span class="st">&#39;w&#39;</span>,<span class="st">&#39;wi&#39;</span>)]</span>
<span id="cb1025-3"><a href="regularized-covariance-matrix.html#cb1025-3" aria-hidden="true" tabindex="-1"></a>gl</span></code></pre></div>
<pre><code>## $w
##               [,1]          [,2]        [,3]         [,4]         [,5]
##  [1,]  1.153311221 -0.0019492424 -0.10250046  0.089115084 -0.365445494
##  [2,] -0.001949242  0.6082630590 -0.03940240  0.004528138  0.011247377
##  [3,] -0.102500456 -0.0394024012  1.73111338 -0.243862447 -0.082905020
##  [4,]  0.089115084  0.0045281381 -0.24386245  1.505275920 -0.027903824
##  [5,] -0.365445494  0.0112473773 -0.08290502 -0.027903824  0.817662287
##  [6,] -0.015688470  0.0005372775 -0.02436263  0.162361394  0.005007582
##  [7,]  0.013032583 -0.0097981855  0.09957364  0.044291165 -0.041565070
##  [8,] -0.045950765  0.0140890956 -0.13200901 -0.132575754  0.112076056
##  [9,]  0.016042995  0.0114506691 -0.23788553  0.263312936  0.026628021
## [10,] -0.040785435  0.0752668494 -0.82444547  0.094745570  0.235337162
##                [,6]         [,7]        [,8]        [,9]       [,10]
##  [1,] -0.0156884699  0.013032583 -0.04595076  0.01604299 -0.04078544
##  [2,]  0.0005372775 -0.009798185  0.01408910  0.01145067  0.07526685
##  [3,] -0.0243626295  0.099573642 -0.13200901 -0.23788553 -0.82444547
##  [4,]  0.1623613944  0.044291165 -0.13257575  0.26331294  0.09474557
##  [5,]  0.0050075816 -0.041565070  0.11207606  0.02662802  0.23533716
##  [6,]  1.0783905237  0.004547886 -0.01346013  0.02839133  0.01124185
##  [7,]  0.0045478865  0.639864046 -0.14066560 -0.02176493 -0.20501465
##  [8,] -0.0134601285 -0.140665596  0.73022332  0.01972098  0.29479653
##  [9,]  0.0283913276 -0.021764935  0.01972098  0.80505626  0.23959079
## [10,]  0.0112418534 -0.205014654  0.29479653  0.23959079  1.57486374
## 
## $wi
##              [,1]       [,2]       [,3]         [,4]         [,5]        [,6]
##  [1,]  1.02453397  0.0000000 0.07730448 -0.041768030  0.464189834  0.02078441
##  [2,]  0.00000000  1.6538060 0.00000000  0.000000000  0.000000000  0.00000000
##  [3,]  0.07729822  0.0000000 0.79699479  0.084060598  0.000000000  0.00000000
##  [4,] -0.04176456  0.0000000 0.08406127  0.743589130  0.001284129 -0.10295591
##  [5,]  0.46418826  0.0000000 0.00000000  0.001284557  1.502048141  0.00000000
##  [6,]  0.02078392  0.0000000 0.00000000 -0.102955958  0.000000000  0.94311116
##  [7,]  0.00000000  0.0000000 0.00000000 -0.038678985  0.000000000  0.00000000
##  [8,]  0.00000000  0.0000000 0.00000000  0.143926311 -0.124728257  0.00000000
##  [9,]  0.00000000  0.0000000 0.08715415 -0.218520346  0.000000000  0.00000000
## [10,]  0.00000000 -0.0790397 0.40091494  0.000000000 -0.189163291  0.00000000
##              [,7]       [,8]        [,9]       [,10]
##  [1,]  0.00000000  0.0000000  0.00000000  0.00000000
##  [2,]  0.00000000  0.0000000  0.00000000 -0.07903969
##  [3,]  0.00000000  0.0000000  0.08715362  0.40091441
##  [4,] -0.03867901  0.1439262 -0.21852024  0.00000000
##  [5,]  0.00000000 -0.1247273  0.00000000 -0.18916474
##  [6,]  0.00000000  0.0000000  0.00000000  0.00000000
##  [7,]  1.67533935  0.2452522  0.00000000  0.17451303
##  [8,]  0.24525219  1.5631718  0.00000000 -0.25070153
##  [9,]  0.00000000  0.0000000  1.38457190 -0.15186887
## [10,]  0.17451306 -0.2507014 -0.15186929  0.96965130</code></pre>
<div class="sourceCode" id="cb1027"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1027-1"><a href="regularized-covariance-matrix.html#cb1027-1" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span><span class="fu">cov2cor</span>(gl<span class="sc">$</span>wi)</span></code></pre></div>
<pre><code>##              [,1]       [,2]        [,3]         [,4]         [,5]        [,6]
##  [1,] -1.00000000  0.0000000 -0.08554877  0.047853564 -0.374188570 -0.02114429
##  [2,]  0.00000000 -1.0000000  0.00000000  0.000000000  0.000000000  0.00000000
##  [3,] -0.08554184  0.0000000 -1.00000000 -0.109193860  0.000000000  0.00000000
##  [4,]  0.04784959  0.0000000 -0.10919473 -1.000000000 -0.001215067  0.12294286
##  [5,] -0.37418730  0.0000000  0.00000000 -0.001215471 -1.000000000  0.00000000
##  [6,] -0.02114379  0.0000000  0.00000000  0.122942921  0.000000000 -1.00000000
##  [7,]  0.00000000  0.0000000  0.00000000  0.034654302  0.000000000  0.00000000
##  [8,]  0.00000000  0.0000000  0.00000000 -0.133496635  0.081399093  0.00000000
##  [9,]  0.00000000  0.0000000 -0.08296646  0.215361272  0.000000000  0.00000000
## [10,]  0.00000000  0.0624159 -0.45605446  0.000000000  0.156742635  0.00000000
##              [,7]        [,8]        [,9]       [,10]
##  [1,]  0.00000000  0.00000000  0.00000000  0.00000000
##  [2,]  0.00000000  0.00000000  0.00000000  0.06241589
##  [3,]  0.00000000  0.00000000 -0.08296595 -0.45605385
##  [4,]  0.03465432 -0.13349651  0.21536117  0.00000000
##  [5,]  0.00000000  0.08139844  0.00000000  0.15674384
##  [6,]  0.00000000  0.00000000  0.00000000  0.00000000
##  [7,] -1.00000000 -0.15155075  0.00000000 -0.13692056
##  [8,] -0.15155077 -1.00000000  0.00000000  0.20363191
##  [9,]  0.00000000  0.00000000 -1.00000000  0.13106997
## [10,] -0.13692058  0.20363183  0.13107034 -1.00000000</code></pre>
<div class="sourceCode" id="cb1029"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1029-1"><a href="regularized-covariance-matrix.html#cb1029-1" aria-hidden="true" tabindex="-1"></a><span class="co"># glassoFast</span></span>
<span id="cb1029-2"><a href="regularized-covariance-matrix.html#cb1029-2" aria-hidden="true" tabindex="-1"></a>glf <span class="ot">&lt;-</span> glassoFast<span class="sc">::</span><span class="fu">glassoFast</span>(S,<span class="at">rho=</span><span class="fl">0.2641</span>)</span>
<span id="cb1029-3"><a href="regularized-covariance-matrix.html#cb1029-3" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span><span class="fu">cov2cor</span>(glf<span class="sc">$</span>wi)</span></code></pre></div>
<pre><code>##              [,1]        [,2]        [,3]         [,4]         [,5]        [,6]
##  [1,] -1.00000000  0.00000000 -0.08554575  0.047851718 -0.374188717 -0.02114399
##  [2,]  0.00000000 -1.00000000  0.00000000  0.000000000  0.000000000  0.00000000
##  [3,] -0.08554575  0.00000000 -1.00000000 -0.109194550  0.000000000  0.00000000
##  [4,]  0.04785172  0.00000000 -0.10919455 -1.000000000 -0.001215165  0.12294285
##  [5,] -0.37418872  0.00000000  0.00000000 -0.001215165 -1.000000000  0.00000000
##  [6,] -0.02114399  0.00000000  0.00000000  0.122942854  0.000000000 -1.00000000
##  [7,]  0.00000000  0.00000000  0.00000000  0.034654196  0.000000000  0.00000000
##  [8,]  0.00000000  0.00000000  0.00000000 -0.133496568  0.081401944  0.00000000
##  [9,]  0.00000000  0.00000000 -0.08296614  0.215361164  0.000000000  0.00000000
## [10,]  0.00000000  0.06241583 -0.45605417  0.000000000  0.156742467  0.00000000
##             [,7]        [,8]        [,9]       [,10]
##  [1,]  0.0000000  0.00000000  0.00000000  0.00000000
##  [2,]  0.0000000  0.00000000  0.00000000  0.06241583
##  [3,]  0.0000000  0.00000000 -0.08296614 -0.45605417
##  [4,]  0.0346542 -0.13349657  0.21536116  0.00000000
##  [5,]  0.0000000  0.08140194  0.00000000  0.15674247
##  [6,]  0.0000000  0.00000000  0.00000000  0.00000000
##  [7,] -1.0000000 -0.15155079  0.00000000 -0.13692053
##  [8,] -0.1515508 -1.00000000  0.00000000  0.20363189
##  [9,]  0.0000000  0.00000000 -1.00000000  0.13107029
## [10,] -0.1369205  0.20363189  0.13107029 -1.00000000</code></pre>
<p>And Ridge:</p>
<div class="sourceCode" id="cb1031"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1031-1"><a href="regularized-covariance-matrix.html#cb1031-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">6</span></span>
<span id="cb1031-2"><a href="regularized-covariance-matrix.html#cb1031-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb1031-3"><a href="regularized-covariance-matrix.html#cb1031-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb1031-4"><a href="regularized-covariance-matrix.html#cb1031-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span> (<span class="fu">rnorm</span>(n<span class="sc">*</span>p), n, p)</span>
<span id="cb1031-5"><a href="regularized-covariance-matrix.html#cb1031-5" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">cov</span>(X)</span>
<span id="cb1031-6"><a href="regularized-covariance-matrix.html#cb1031-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1031-7"><a href="regularized-covariance-matrix.html#cb1031-7" aria-hidden="true" tabindex="-1"></a><span class="co"># corpcor</span></span>
<span id="cb1031-8"><a href="regularized-covariance-matrix.html#cb1031-8" aria-hidden="true" tabindex="-1"></a>cpr <span class="ot">&lt;-</span> corpcor<span class="sc">::</span><span class="fu">pcor.shrink</span>(X)</span></code></pre></div>
<pre><code>## Estimating optimal shrinkage intensity lambda (correlation matrix): 0.7365</code></pre>
<div class="sourceCode" id="cb1033"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1033-1"><a href="regularized-covariance-matrix.html#cb1033-1" aria-hidden="true" tabindex="-1"></a>cpr</span></code></pre></div>
<pre><code>##              [,1]         [,2]         [,3]         [,4]         [,5]
##  [1,]  1.00000000 -0.063781216 -0.101406692  0.063120988 -0.194493134
##  [2,] -0.06378122  1.000000000  0.007994094 -0.000768278  0.045367893
##  [3,] -0.10140669  0.007994094  1.000000000 -0.079446872  0.016525671
##  [4,]  0.06312099 -0.000768278 -0.079446872  1.000000000 -0.062013066
##  [5,] -0.19449313  0.045367893  0.016525671 -0.062013066  1.000000000
##  [6,] -0.08453161 -0.142765543 -0.023156121  0.118733605  0.101284666
##  [7,]  0.01012585 -0.066403162  0.078386268  0.085900860 -0.090559866
##  [8,] -0.05718524  0.062980841 -0.073635683 -0.106514843  0.121121615
##  [9,] -0.01423788  0.137888867 -0.113559683  0.147784876 -0.001232215
## [10,] -0.03863764  0.081744909 -0.163495477  0.045875669  0.096163827
##              [,6]        [,7]        [,8]         [,9]       [,10]
##  [1,] -0.08453161  0.01012585 -0.05718524 -0.014237882 -0.03863764
##  [2,] -0.14276554 -0.06640316  0.06298084  0.137888867  0.08174491
##  [3,] -0.02315612  0.07838627 -0.07363568 -0.113559683 -0.16349548
##  [4,]  0.11873361  0.08590086 -0.10651484  0.147784876  0.04587567
##  [5,]  0.10128467 -0.09055987  0.12112162 -0.001232215  0.09616383
##  [6,]  1.00000000  0.02022953 -0.01359508 -0.016265809  0.04534151
##  [7,]  0.02022953  1.00000000 -0.18507676  0.050393624 -0.11283317
##  [8,] -0.01359508 -0.18507676  1.00000000 -0.031721445  0.12046550
##  [9,] -0.01626581  0.05039362 -0.03172144  1.000000000  0.12181207
## [10,]  0.04534151 -0.11283317  0.12046550  0.121812073  1.00000000
## attr(,&quot;lambda&quot;)
## [1] 0.7364502
## attr(,&quot;lambda.estimated&quot;)
## [1] TRUE
## attr(,&quot;class&quot;)
## [1] &quot;shrinkage&quot;
## attr(,&quot;spv&quot;)
##  [1] 0.9163043 0.9261594 0.9159745 0.9151906 0.8824546 0.9457673 0.8900877
##  [8] 0.8719793 0.9159940 0.8735017</code></pre>
<div class="sourceCode" id="cb1035"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1035-1"><a href="regularized-covariance-matrix.html#cb1035-1" aria-hidden="true" tabindex="-1"></a><span class="co"># rags2ridges</span></span>
<span id="cb1035-2"><a href="regularized-covariance-matrix.html#cb1035-2" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">&lt;-</span> rags2ridges<span class="sc">::</span><span class="fu">optPenalty.kCVauto</span>(X, <span class="at">lambdaMin =</span> <span class="fl">0.01</span>, <span class="at">lambdaMax =</span> <span class="dv">30</span>)</span>
<span id="cb1035-3"><a href="regularized-covariance-matrix.html#cb1035-3" aria-hidden="true" tabindex="-1"></a>opt</span></code></pre></div>
<pre><code>## $optLambda
## [1] 3.670373
## 
## $optPrec
## A 10 x 10 ridge precision matrix estimate with lambda = 3.670373
##             [,1]         [,2]        [,3]         [,4]         [,5]        [,6] â¦
## [1,]  2.07949562  0.036633616 0.078479888 -0.074714740  0.133955431  0.05979074 â¦
## [2,]  0.03663362  2.195421841 0.019061940  0.009073605 -0.029391003  0.05604419 â¦
## [3,]  0.07847989  0.019061940 1.958521840  0.107011697  0.004540494  0.02397844 â¦
## [4,] -0.07471474  0.009073605 0.107011697  2.004976897  0.062008722 -0.09090979 â¦
## [5,]  0.13395543 -0.029391003 0.004540494  0.062008722  2.151438333 -0.05607991 â¦
## [6,]  0.05979074  0.056044189 0.023978441 -0.090909787 -0.056079914  2.09478151 â¦
## â¦ 4 more rows and 4 more columns</code></pre>
<div class="sourceCode" id="cb1037"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1037-1"><a href="regularized-covariance-matrix.html#cb1037-1" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span><span class="fu">cov2cor</span>(opt<span class="sc">$</span>optPrec)</span></code></pre></div>
<pre><code>## A 10 x 10 ridge precision matrix estimate with lambda = 3.670373
##             [,1]         [,2]         [,3]         [,4]         [,5] â¦
## [1,] -1.00000000 -0.017145170 -0.038887960  0.036590858 -0.063331077 â¦
## [2,] -0.01714517 -1.000000000 -0.009192717 -0.004324802  0.013523558 â¦
## [3,] -0.03888796 -0.009192717 -1.000000000 -0.054002312 -0.002211946 â¦
## [4,]  0.03659086 -0.004324802 -0.054002312 -1.000000000 -0.029856139 â¦
## [5,] -0.06333108  0.013523558 -0.002211946 -0.029856139 -1.000000000 â¦
## [6,] -0.02864742 -0.026133780 -0.011838246  0.044359494  0.026416393 â¦
##             [,6] â¦
## [1,] -0.02864742 â¦
## [2,] -0.02613378 â¦
## [3,] -0.01183825 â¦
## [4,]  0.04435949 â¦
## [5,]  0.02641639 â¦
## [6,] -1.00000000 â¦
## â¦ 4 more rows and 4 more columns</code></pre>
<div class="sourceCode" id="cb1039"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1039-1"><a href="regularized-covariance-matrix.html#cb1039-1" aria-hidden="true" tabindex="-1"></a>Si <span class="ot">&lt;-</span> rags2ridges<span class="sc">::</span><span class="fu">ridgeP</span>(S, <span class="at">lambda=</span>opt<span class="sc">$</span>optLambda)</span>
<span id="cb1039-2"><a href="regularized-covariance-matrix.html#cb1039-2" aria-hidden="true" tabindex="-1"></a>Si</span></code></pre></div>
<pre><code>## A 10 x 10 ridge precision matrix estimate with lambda = 3.670373
##             [,1]        [,2]        [,3]        [,4]         [,5]        [,6] â¦
## [1,]  1.71127935  0.04250957 0.091807424 -0.08619735  0.155561843  0.06978510 â¦
## [2,]  0.04250957  1.84587476 0.021184684  0.01052115 -0.033800447  0.06587097 â¦
## [3,]  0.09180742  0.02118468 1.573665969  0.12278373  0.004278519  0.02711906 â¦
## [4,] -0.08619735  0.01052115 0.122783725  1.62525272  0.071878763 -0.10579069 â¦
## [5,]  0.15556184 -0.03380045 0.004278519  0.07187876  1.795540422 -0.06533860 â¦
## [6,]  0.06978510  0.06587097 0.027119058 -0.10579069 -0.065338597  1.72817503 â¦
## â¦ 4 more rows and 4 more columns</code></pre>
<div class="sourceCode" id="cb1041"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1041-1"><a href="regularized-covariance-matrix.html#cb1041-1" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span><span class="fu">cov2cor</span>(Si)</span></code></pre></div>
<pre><code>## A 10 x 10 ridge precision matrix estimate with lambda = 3.670373
##             [,1]         [,2]         [,3]         [,4]         [,5] â¦
## [1,] -1.00000000 -0.023918016 -0.055944973  0.051686010 -0.088745232 â¦
## [2,] -0.02391802 -1.000000000 -0.012429813 -0.006074373  0.018566230 â¦
## [3,] -0.05594497 -0.012429813 -1.000000000 -0.076775752 -0.002545304 â¦
## [4,]  0.05168601 -0.006074373 -0.076775752 -1.000000000 -0.042076785 â¦
## [5,] -0.08874523  0.018566230 -0.002545304 -0.042076785 -1.000000000 â¦
## [6,] -0.04057966 -0.036880696 -0.016444644  0.063123798  0.037091838 â¦
##             [,6] â¦
## [1,] -0.04057966 â¦
## [2,] -0.03688070 â¦
## [3,] -0.01644464 â¦
## [4,]  0.06312380 â¦
## [5,]  0.03709184 â¦
## [6,] -1.00000000 â¦
## â¦ 4 more rows and 4 more columns</code></pre>
<div class="sourceCode" id="cb1043"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1043-1"><a href="regularized-covariance-matrix.html#cb1043-1" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span>Si[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span>(<span class="fu">sqrt</span>(Si[<span class="dv">1</span>,<span class="dv">1</span>])<span class="sc">*</span><span class="fu">sqrt</span>(Si[<span class="dv">2</span>,<span class="dv">2</span>])) </span></code></pre></div>
<pre><code>## [1] -0.02391802</code></pre>
<div class="sourceCode" id="cb1045"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1045-1"><a href="regularized-covariance-matrix.html#cb1045-1" aria-hidden="true" tabindex="-1"></a>spr <span class="ot">&lt;-</span> rags2ridges<span class="sc">::</span><span class="fu">sparsify</span>(opt<span class="sc">$</span>optPrec, <span class="at">threshold =</span> <span class="st">&quot;connected&quot;</span>)</span></code></pre></div>
<pre><code>## - Retained elements:  16 
## - Corresponding to 35.56 % of possible edges 
## </code></pre>
<div class="sourceCode" id="cb1047"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1047-1"><a href="regularized-covariance-matrix.html#cb1047-1" aria-hidden="true" tabindex="-1"></a>spr</span></code></pre></div>
<pre><code>## $sparseParCor
## A 10 x 10 ridge precision matrix estimate with lambda = 3.670373
##             [,1] [,2]        [,3]        [,4]        [,5]       [,6] â¦
## [1,]  1.00000000    0 -0.03888796  0.03659086 -0.06333108 0.00000000 â¦
## [2,]  0.00000000    1  0.00000000  0.00000000  0.00000000 0.00000000 â¦
## [3,] -0.03888796    0  1.00000000 -0.05400231  0.00000000 0.00000000 â¦
## [4,]  0.03659086    0 -0.05400231  1.00000000  0.00000000 0.04435949 â¦
## [5,] -0.06333108    0  0.00000000  0.00000000  1.00000000 0.00000000 â¦
## [6,]  0.00000000    0  0.00000000  0.04435949  0.00000000 1.00000000 â¦
## â¦ 4 more rows and 4 more columns
## 
## $sparsePrecision
## A 10 x 10 ridge precision matrix estimate with lambda = 3.670373
##             [,1]     [,2]       [,3]        [,4]      [,5]        [,6] â¦
## [1,]  2.07949562 0.000000 0.07847989 -0.07471474 0.1339554  0.00000000 â¦
## [2,]  0.00000000 2.195422 0.00000000  0.00000000 0.0000000  0.00000000 â¦
## [3,]  0.07847989 0.000000 1.95852184  0.10701170 0.0000000  0.00000000 â¦
## [4,] -0.07471474 0.000000 0.10701170  2.00497690 0.0000000 -0.09090979 â¦
## [5,]  0.13395543 0.000000 0.00000000  0.00000000 2.1514383  0.00000000 â¦
## [6,]  0.00000000 0.000000 0.00000000 -0.09090979 0.0000000  2.09478151 â¦
## â¦ 4 more rows and 4 more columns</code></pre>
<div class="sourceCode" id="cb1049"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1049-1"><a href="regularized-covariance-matrix.html#cb1049-1" aria-hidden="true" tabindex="-1"></a>rags2ridges<span class="sc">::</span><span class="fu">GGMnetworkStats</span>(spr<span class="sc">$</span>sparseParCor, <span class="at">as.table =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>##       degree betweenness  closeness eigenCentrality nNeg nPos  mutualInfo
##  [1,]      3    1.500000 0.05882353       0.6272576    2    1 0.006771080
##  [2,]      1    0.000000 0.04761905       0.2661848    0    1 0.001206143
##  [3,]      4    3.083333 0.07142857       0.8538060    4    0 0.019887533
##  [4,]      5   10.250000 0.07142857       0.8583712    2    3 0.010460261
##  [5,]      3    2.000000 0.06250000       0.6442970    1    2 0.007847334
##  [6,]      1    0.000000 0.04545455       0.2284853    0    1 0.001986482
##  [7,]      2    0.000000 0.05555556       0.4773306    2    0 0.003664628
##  [8,]      4    5.333333 0.07142857       0.7932303    2    2 0.007450243
##  [9,]      3    1.333333 0.06666667       0.7219403    1    2 0.007439663
## [10,]      6   13.500000 0.07692308       1.0000000    2    4 0.024757858
##       variance partialVar
##  [1,] 1.006794          1
##  [2,] 1.001207          1
##  [3,] 1.020087          1
##  [4,] 1.010515          1
##  [5,] 1.007878          1
##  [6,] 1.001988          1
##  [7,] 1.003671          1
##  [8,] 1.007478          1
##  [9,] 1.007467          1
## [10,] 1.025067          1</code></pre>
<div class="sourceCode" id="cb1051"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1051-1"><a href="regularized-covariance-matrix.html#cb1051-1" aria-hidden="true" tabindex="-1"></a><span class="co">#rags2ridges::fullMontyS(X, lambdaMin = 0.01, lambdaMax = 30) - Gives an error</span></span>
<span id="cb1051-2"><a href="regularized-covariance-matrix.html#cb1051-2" aria-hidden="true" tabindex="-1"></a>rags2ridges<span class="sc">::</span><span class="fu">Ugraph</span>(spr<span class="sc">$</span>sparseParCor, <span class="at">type =</span> <span class="st">&quot;weighted&quot;</span>)</span></code></pre></div>
<pre><code>## Warning in type.convert.default(X[[i]], ...): &#39;as.is&#39; should be specified by the
## caller; using TRUE

## Warning in type.convert.default(X[[i]], ...): &#39;as.is&#39; should be specified by the
## caller; using TRUE</code></pre>
<p><img src="YA_TextBook_files/figure-html/unnamed-chunk-326-1.png" width="672" /></p>
<pre><code>##            [,1]          [,2]
##  [1,]  1.000000  0.000000e+00
##  [2,]  0.809017  5.877853e-01
##  [3,]  0.309017  9.510565e-01
##  [4,] -0.309017  9.510565e-01
##  [5,] -0.809017  5.877853e-01
##  [6,] -1.000000  1.224647e-16
##  [7,] -0.809017 -5.877853e-01
##  [8,] -0.309017 -9.510565e-01
##  [9,]  0.309017 -9.510565e-01
## [10,]  0.809017 -5.877853e-01</code></pre>

</div>
</div>



<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Intui_Cross" class="csl-entry">
Lepidopterist. 2015. <span>âWhat Is the Intuitive (Geometric?) Meaning of Minimizing the Log Determinant of a Matrix?â</span> Cross Validated. <a href="https://stats.stackexchange.com/q/151315">https://stats.stackexchange.com/q/151315</a>.
</div>
<div id="ref-Schafer_2005" class="csl-entry">
SchÃ¤fer, Juliane, and Korbinian Strimmer. 2005. <span>âA Shrinkage Approach to Large-Scale Covariance Matrix Estimation and Implications for Functional Genomics.â</span> <em>Statistical Applications in Genetic &amp; Molecular Biology</em> 4 (32). <a href="https://pubmed.ncbi.nlm.nih.gov/16646851/">https://pubmed.ncbi.nlm.nih.gov/16646851/</a>.
</div>
<div id="ref-Wessel_2019" class="csl-entry">
Wieringen, Wessel van. 2019. <span>âThe Generalized Ridge Estimator of the Inverse Covariance Matrix.â</span> <em>Journal of Computational &amp; Graphical Statistics</em> 28 (4): 932â42. <a href="https://www.tandfonline.com/doi/epub/10.1080/10618600.2019.1604374?needAccess=true https://www.sciencedirect.com/science/article/abs/pii/S0167947316301141 ">https://www.tandfonline.com/doi/epub/10.1080/10618600.2019.1604374?needAccess=true https://www.sciencedirect.com/science/article/abs/pii/S0167947316301141 </a>.
</div>
<div id="ref-Wessel_Ridge" class="csl-entry">
Wieringen, Wessel van, and Carel F. W. Peeters. 2019. <span>âRidge Estimation of Inverse Covariance Matrices from High-Dimensional Data.â</span> <em>Computational Statistics &amp; Data Analysis</em> 103: 284â303. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167947316301141">https://www.sciencedirect.com/science/article/abs/pii/S0167947316301141</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="18">
<li id="fn18"><p>See: <a href="https://www.tandfonline.com/doi/epub/10.1080/10618600.2019.1604374?needAccess=true">The Generalized Ridge Estimator of the Inverse Covariance Matrix</a> <span class="citation">(<a href="#ref-Wessel_2019" role="doc-biblioref">Wieringen 2019</a>)</span>, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167947316301141">Ridge estimation of inverse covariance matrices from high-dimensional data</a> <span class="citation">(<a href="#ref-Wessel_Ridge" role="doc-biblioref">Wieringen and Peeters 2019</a>)</span><a href="regularized-covariance-matrix.html#fnref18" class="footnote-back">â©ï¸</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="fundementals.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="r-lab-1---basics-i.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/yaydede/ToolShed/edit/master/32-RegularizedCovMatrix.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["YA_TextBook.pdf", "YA_TextBook.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
