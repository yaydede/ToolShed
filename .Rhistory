lambda = grid, thresh = 1e-12)
set.seed(1)
cv.outL <- cv.glmnet(X[train,], y[train], alpha = 1)
bestlamL <- cv.outL$lambda.min
yhatL <- predict(lasso.mod, s = bestlamL, newx = X[test,])
mse_L <- mean((yhatL - ytest)^2)
mse_R
mse_L
grid = 10^seq(10, -2, length = 100)
MSPE <- c()
MMSPE <- c()
for(i in 1:length(grid)){
for(j in 1:100){
set.seed(j)
ind <- unique(sample(nrow(df), nrow(df), replace = TRUE))
train <- df[ind, ]
xtrain <- model.matrix(Salary~., train)[,-1]
ytrain <- df[ind, 19]
test <- df[-ind, ]
xtest <- model.matrix(Salary~., test)[,-1]
ytest <- df[-ind, 19]
model <- glmnet(xtrain, ytrain, alpha = 1,
lambda = grid[i], thresh = 1e-12)
yhat <- predict(model, s = grid[i], newx = xtest)
MSPE[j] <- mean((yhat - ytest)^2)
}
MMSPE[i] <- mean(MSPE)
}
min(MMSPE)
grid[which.min(MMSPE)]
plot(log(grid), MMSPE, type="o", col = "red", lwd = 3)
coef_lasso <- coef(model, s=grid[which.min(MMSPE)], nonzero = T)
coef_lasso
library(PASWR)
remove(list = ls())
data(titanic3)
str(titanic3)
# Data
a <- c("survived","sex","age","pclass","sibsp","parch")
df <- titanic3[, a]
df <- df[complete.cases(df), ]
df$survived <- as.factor(df$survived)
# model.matrix
X  <- model.matrix(survived~., df)[,-1]
y <- df$survived
library(PASWR)
remove(list = ls())
data(titanic3)
str(titanic3)
# Selected features
a <- c("survived","sex","age","pclass","sibsp","parch")
df <- titanic3[, a]
df <- df[complete.cases(df), ]
df$survived <- as.factor(df$survived)
# model.matrix
X  <- model.matrix(survived~., df)[,-1]
y <- df$survived
View(X)
library(PASWR)
remove(list = ls())
data(titanic3)
# Selected features
a <- c("survived","sex","age","pclass","sibsp","parch")
df <- titanic3[, a]
df <- df[complete.cases(df), ]
df$survived <- as.factor(df$survived)
str(df)
X  <- model.matrix(survived~., df)[,-1]
y <- df$survived
View(X)
View(titanic3)
set.seed(1)
train <- sample(nrow(X), nrow(X)*0.5)
ytest <- y[-train]
length(ytest)
length(y)
ytest
tr <- df[ind, ]
set.seed(1)
ind <- unique(sample(nrow(X), nrow(X)*0.5))
ytrain <- y[ind]
Xtrain <- X[ind,]
ytest <- y[-ind]
Xtest <- X[-ind,]
set.seed(1)
ind <- sample(nrow(X), nrow(X)*0.5)
ytrain <- y[ind]
Xtrain <- X[ind,]
ytest <- y[-ind]
Xtest <- X[-ind,]
tr <- df[ind, ]
tt <- df[-ind, ]
library(ROCR)
# model.matrix
X  <- model.matrix(survived~., df)[,-1]
y <- df$survived
set.seed(1)
ind <- sample(nrow(X), nrow(X)*0.5)
ytrain <- y[ind]
Xtrain <- X[ind,]
ytest <- y[-ind]
Xtest <- X[-ind,]
# Ridge
set.seed(1)
cv.outR <- cv.glmnet(Xtrain, ytrain, alpha = 0, family = "binomial")
phatR <- predict(cv.outR, s = "lambda.min",
newx = Xtest, type = "response")
predR <- prediction(phatR, ytest)
aucR <- performance(predR, measure = "auc")
AUCR <- aucR@y.values[[1]]
# Lasso
set.seed(1)
cv.outL <- cv.glmnet(Xtrain, ytrain, alpha = 1, family = "binomial")
phatL <- predict(cv.outL, s = "lambda.min", newx = Xtest, type = "response")
predL <- prediction(phatL, ytest)
aucL <- performance(predL, measure = "auc")
AUCL <- aucL@y.values[[1]]
AUCR
AUCL
#Logistic
tr <- df[ind, ]
tt <- df[-ind, ]
logi <- glm(survived~., data = tr, family = "binomial")
phat <- predict(logi, newdata = tt, type = "response")
pred <- prediction(phat, tt$survived)
auc <- performance(pred, measure = "auc")
auc@y.values[[1]]
View(df)
grid = 10^seq(10, -2, length = 100)
AUC <- c()
MAUC <- c()
for(i in 1:length(grid)){
for(j in 1:100){
set.seed(j)
ind <- unique(sample(nrow(df), nrow(df), replace = TRUE))
xtrain <- model.matrix(survived~., df[ind, ])[,-1]
ytrain <- df[ind, 1]
xtest <- model.matrix(survived~., df[-ind, ])[,-1]
ytest <- df[-ind, 1]
model <- glmnet(xtrain, ytrain, alpha = 1, lambda = grid[i],
thresh = 1e-12, family = "binomial")
phat <- predict(model, s = grid[i], newx = xtest, type = "response")
pred <- prediction(phat, ytest)
auc <- performance(pred, measure = "auc")
AUC[j] <- auc@y.values[[1]]
}
MAUC[i] <- mean(AUC)
}
max(MAUC)
grid[which.max(MAUC)]
plot(log(grid), MAUC, type="o", col = "red", lwd = 3)
library(ISLR)
library(glmnet)
remove(list = ls())
data(Hitters)
df <- Hitters[complete.cases(Hitters$Salary), ]
X  <- model.matrix(Salary~., df)[,-1]
y <- df$Salary
set.seed(1)
modelr <- cv.glmne(X, y, alpha = 0)
modelr <- cv.glmnet(X, y, alpha = 0)
w <- 1/abs(matrix(coef(modelr, s = modelr$lambda.min)[, 1][2:(ncol(x)+1)] ))^1
w <- 1/abs(matrix(coef(modelr, s = modelr$lambda.min)[, 1][2:(ncol(X)+1)] ))^1 ## Using gamma = 1
set.seed(1)
modelr <- cv.glmnet(X, y, alpha = 0)
w <- 1/abs(matrix(coef(modelr, s = modelr$lambda.min)[, 1][2:(ncol(X)+1)] ))^1 # gamma = 1
w[w[,1] == Inf] <- 999999 ## Replacing Infinite with 9999999
set.seed(1)
alasso <- cv.glmnet(X, y, alpha=1, penalty.factor = w)
plot(alasso)
plot(alasso$glmnet.fit, xvar="lambda", label=TRUE)
plot(alasso)
plot(alasso$glmnet.fit, xvar="lambda", label=TRUE)
abline(v = log(alasso$lambda.min))
abline(v = log(alasso$lambda.1se))
plot(alasso$glmnet.fit, xvar="lambda", label=TRUE)
coef(alasso, s=alasso$lambda.1se)
coef <- coef(alasso, s='lambda.1se')
(coef@i[-1]+1)
coef(alasso, s=alasso$lambda.1se)
coef <- coef(alasso, s='lambda.1se')
(coef@i[-1]+1)
coef(alasso, s='lambda.1se')
coef(alasso, s=alasso$lambda.1se)
w <- 1/abs(matrix(coef(modelr, s = modelr$lambda.min)[, 1][2:(ncol(X)+1)] ))^1 # gamma = 1
w
1/abs(matrix(coef(modelr, s = "lambda.min")
coef(modelr, s = "lambda.min")
coef(modelr, s = modelr$lambda.min)
as.matrix(coef(modelr, s = modelr$lambda.min))
as.matrix(coef(modelr, s = modelr$lambda.min))[-1,]
coefr <- as.matrix(coef(modelr, s = modelr$lambda.min))
coefr[-1,]
coefr[-1,1]
1/abs(coefr[-1,])
w
t(1/abs(coefr[-1,]))
w.r <- 1/abs(coefr[-1,])
alasso <- cv.glmnet(X, y, alpha=1, penalty.factor = w.r)
coef(alasso, s="lambda.1se")
g = 1
w.r <- 1/(abs(coefr[-1,]))^g
library(ISLR)
library(glmnet)
remove(list = ls())
data(Hitters)
df <- Hitters[complete.cases(Hitters$Salary), ]
X  <- model.matrix(Salary~., df)[,-1]
y <- df$Salary
# Ridge with gamma = 1
g = 1
set.seed(1)
modelr <- cv.glmnet(X, y, alpha = 0)
coefr <- as.matrix(coef(modelr, s = modelr$lambda.min))
w.r <- 1/(abs(coefr[-1,]))^g
## Adaptive Lasso
set.seed(1)
alasso <- cv.glmnet(X, y, alpha=1, penalty.factor = w.r)
## Lasso
set.seed(1)
lasso <- cv.glmnet(X, y, alpha=1)
coef(lasso, s="lambda.1se")
coef(alasso, s="lambda.1se")
library(ISLR)
library(glmnet)
remove(list = ls())
data(Hitters)
df <- Hitters[complete.cases(Hitters$Salary), ]
X  <- model.matrix(Salary~., df)[,-1]
y <- df$Salary
# Ridge with gamma = 1
g = 1
set.seed(1)
modelr <- cv.glmnet(X, y, alpha = 0)
coefr <- as.matrix(coef(modelr, s = modelr$lambda.min))
w.r <- 1/(abs(coefr[-1,]))^g
## Adaptive Lasso
set.seed(1)
alasso <- cv.glmnet(X, y, alpha=1, penalty.factor = w.r)
## Lasso
set.seed(1)
lasso <- cv.glmnet(X, y, alpha=1)
data.frame(LASSO = coef(lasso, s="lambda.1se"),
ALASSO = coef(alasso, s="lambda.1se"))
coef(lasso, s="lambda.1se")
cbind(LASSO = coef(lasso, s="lambda.1se"),
ALASSO = coef(alasso, s="lambda.1se"))
gitcreds::gitcreds_set()
library(glmnet)
library(ISLR)
remove(list = ls())
data(Hitters)
df <- Hitters[complete.cases(Hitters$Salary), ]
X  <- model.matrix(Salary~., df)[,-1]
y <- df$Salary
# Without a specific grid on lambda
set.seed(1)
train <- sample(1:nrow(X), nrow(X)*0.5)
test <- c(-train)
ytest <- y[test]
# Ridge
set.seed(1)
ridge.out <- cv.glmnet(X[train,], y[train], alpha = 0)
yhatR <- predict(ridge.out, s = "lambda.min", newx = X[test,])
mse_r <- mean((yhatR - ytest)^2)
# Lasso
set.seed(1)
lasso.out <- cv.glmnet(X[train,], y[train], alpha = 1)
yhatL <- predict(lasso.out, s = "lambda.min", newx = X[test,])
mse_l <- mean((yhatL - ytest)^2)
mse_r
mse_l
# With a specific grid on lambda + lm()
grid = 10^seq(10, -2, length = 100)
set.seed(1)
train <- sample(1:nrow(X), nrow(X)*0.5)
test <- c(-train)
ytest <- y[test]
#Ridge
ridge.mod <- glmnet(X[train,], y[train], alpha = 0,
lambda = grid, thresh = 1e-12)
set.seed(1)
cv.outR <- cv.glmnet(X[train,], y[train], alpha = 0)
bestlamR <- cv.outR$lambda.min
yhatR <- predict(ridge.mod, s = bestlamR, newx = X[test,])
mse_R <- mean((yhatR - ytest)^2)
# Lasso
lasso.mod <- glmnet(X[train,], y[train], alpha = 1,
lambda = grid, thresh = 1e-12)
set.seed(1)
cv.outL <- cv.glmnet(X[train,], y[train], alpha = 1)
bestlamL <- cv.outL$lambda.min
yhatL <- predict(lasso.mod, s = bestlamL, newx = X[test,])
mse_L <- mean((yhatL - ytest)^2)
mse_R
mse_L
grid = 10^seq(10, -2, length = 100)
MSPE <- c()
MMSPE <- c()
for(i in 1:length(grid)){
for(j in 1:100){
set.seed(j)
ind <- unique(sample(nrow(df), nrow(df), replace = TRUE))
train <- df[ind, ]
xtrain <- model.matrix(Salary~., train)[,-1]
ytrain <- df[ind, 19]
test <- df[-ind, ]
xtest <- model.matrix(Salary~., test)[,-1]
ytest <- df[-ind, 19]
model <- glmnet(xtrain, ytrain, alpha = 1,
lambda = grid[i], thresh = 1e-12)
yhat <- predict(model, s = grid[i], newx = xtest)
MSPE[j] <- mean((yhat - ytest)^2)
}
MMSPE[i] <- mean(MSPE)
}
min(MMSPE)
grid[which.min(MMSPE)]
plot(log(grid), MMSPE, type="o", col = "red", lwd = 3)
plot(log(grid), MMSPE, type="o", col = "red", lwd = 3)
plot(log(grid), MMSPE, type="o", col = "red", lwd = 1)
library(ROCR)
library(glmnet)
# model.matrix
X  <- model.matrix(survived~., df)[,-1]
library(PASWR)
remove(list = ls())
data(titanic3)
# Selected features
a <- c("survived","sex","age","pclass","sibsp","parch")
df <- titanic3[, a]
df <- df[complete.cases(df), ]
df$survived <- as.factor(df$survived)
str(df)
rm(list = ls())
load("~/Dropbox/TheBOOK/Kyle/Kyle Edits (v2)/Pre-Review Edited Version(V2)/Pre-Review Edited Version/creditcard10.RData")
creditcard10 <- df
df <- creditcard10
df$Class <- as.factor(df$Class)
AUCb <- c()
AUCimb <- c()
n = 10 # Could be 50, since the data is large for RF
B = 100
for (i in 1:n) {
set.seed(i+i*100)
ind <- sample(nrow(df), nrow(df), replace = TRUE)
ind <- unique(ind) # Otherwise it oversamples 0's
train <- df[ind, ]
test <- df[-ind, ]
# Balancing
outdf <- SMOTE(X = train[, -31], target = train$Class, K = 10, dup_size = 50)
trainn <- outdf$data
trainn$class <- as.factor(trainn$class) #SMOTE makes factor to "chr"!
colnames(trainn)[31] <- "Class" #SMOTE made it lower case!
modelb <- randomForest(Class~., ntree = B, data = trainn)
phatb <- predict(modelb, test, type = "prob")
# Without Balancing
modelimb <- randomForest(Class~., ntree = B, data = train)
phatimb <- predict(modelimb, test, type = "prob")
#AUCb
pred_rocr1 <- prediction(phatb[,2], test$Class)
auc_ROCR1 <- performance(pred_rocr1, measure = "auc")
AUCb[i] <- auc_ROCR1@y.values[[1]]
#AUCimb
pred_rocr1 <- prediction(phatimb[,2], test$Class)
auc_ROCR1 <- performance(pred_rocr1, measure = "auc")
AUCimb[i] <- auc_ROCR1@y.values[[1]]
}
library(tidyverse)
library(ROCR)
library(smotefamily)
library(randomForest)
head(creditcard10)
table(creditcard10$Class)
prop.table(table(creditcard10$Class))
df <- creditcard10
df$Class <- as.factor(df$Class)
AUCb <- c()
AUCimb <- c()
n = 10 # Could be 50, since the data is large for RF
B = 100
for (i in 1:n) {
set.seed(i+i*100)
ind <- sample(nrow(df), nrow(df), replace = TRUE)
ind <- unique(ind) # Otherwise it oversamples 0's
train <- df[ind, ]
test <- df[-ind, ]
# Balancing
outdf <- SMOTE(X = train[, -31], target = train$Class, K = 10, dup_size = 50)
trainn <- outdf$data
trainn$class <- as.factor(trainn$class) #SMOTE makes factor to "chr"!
colnames(trainn)[31] <- "Class" #SMOTE made it lower case!
modelb <- randomForest(Class~., ntree = B, data = trainn)
phatb <- predict(modelb, test, type = "prob")
# Without Balancing
modelimb <- randomForest(Class~., ntree = B, data = train)
phatimb <- predict(modelimb, test, type = "prob")
#AUCb
pred_rocr1 <- prediction(phatb[,2], test$Class)
auc_ROCR1 <- performance(pred_rocr1, measure = "auc")
AUCb[i] <- auc_ROCR1@y.values[[1]]
#AUCimb
pred_rocr1 <- prediction(phatimb[,2], test$Class)
auc_ROCR1 <- performance(pred_rocr1, measure = "auc")
AUCimb[i] <- auc_ROCR1@y.values[[1]]
}
model <- c("Balanced", "Imbalanced")
AUCs <- c(mean(AUCb), mean(AUCimb))
sd <- c(sqrt(var(AUCb)), sqrt(var(AUCimb)))
data.frame(model, AUCs, sd)
rm(list = ls())
load("creditcard10.RData")
creditcard10 <- df
rm(list = ls())
load("creditcard10.RData")
creditcard10 <- df
is.atomic(train[, -31])
rm(list = ls())
load("creditcard10.RData")
df$Class <- as.factor(df$Class)
ind <- sample(nrow(df), nrow(df), replace = TRUE)
ind <- unique(ind) # Otherwise it oversamples 0's
train <- df[ind, ]
test <- df[-ind, ]
is.atomic(train[, -31])
is.atomic(outdf$data)
outdf <- SMOTE(X = train[, -31], target = train$Class, K = 10, dup_size = 50)
is.atomic(outdf$data)
is.atomic(trainn$class)
trainn <- outdf$data
trainn$class <- as.factor(trainn$class) #SMOTE makes factor to "chr"!
is.atomic(trainn$class)
remotes::install_github('rstudio/bookdown')
rm(list = ls())
load("creditcard10.RData")
df$Class <- as.factor(df$Class)
AUCb <- c()
AUCimb <- c()
n = 10 # Could be 50, since the data is large for RF
B = 100
for (i in 1:n) {
set.seed(i)
ind <- sample(nrow(df), nrow(df), replace = TRUE)
ind <- unique(ind) # Otherwise it oversamples 0's
train <- df[ind, ]
test <- df[-ind, ]
# Balancing
outdf <- SMOTE(X = train[, -31], target = train$Class, K = 10, dup_size = 50)
trainn <- outdf$data
trainn$class <- as.factor(trainn$class) #SMOTE makes factor to "chr"!
colnames(trainn)[31] <- "Class" #SMOTE made it lower case!
modelb <- randomForest(Class~., ntree = B, data = trainn)
phatb <- predict(modelb, test, type = "prob")
# Without Balancing
modelimb <- randomForest(Class~., ntree = B, data = train)
phatimb <- predict(modelimb, test, type = "prob")
#AUCb
pred_rocr1 <- prediction(phatb[,2], test$Class)
auc_ROCR1 <- performance(pred_rocr1, measure = "auc")
AUCb[i] <- auc_ROCR1@y.values[[1]]
#AUCimb
pred_rocr1 <- prediction(phatimb[,2], test$Class)
auc_ROCR1 <- performance(pred_rocr1, measure = "auc")
AUCimb[i] <- auc_ROCR1@y.values[[1]]
}
library(tidyverse)
library(ROCR)
library(smotefamily)
library(randomForest)
df$Class <- as.factor(df$Class)
AUCb <- c()
AUCimb <- c()
n = 10 # Could be 50, since the data is large for RF
B = 100
for (i in 1:n) {
set.seed(i)
ind <- sample(nrow(df), nrow(df), replace = TRUE)
ind <- unique(ind) # Otherwise it oversamples 0's
train <- df[ind, ]
test <- df[-ind, ]
# Balancing
outdf <- SMOTE(X = train[, -31], target = train$Class, K = 10, dup_size = 50)
trainn <- outdf$data
trainn$class <- as.factor(trainn$class) #SMOTE makes factor to "chr"!
colnames(trainn)[31] <- "Class" #SMOTE made it lower case!
modelb <- randomForest(Class~., ntree = B, data = trainn)
phatb <- predict(modelb, test, type = "prob")
# Without Balancing
modelimb <- randomForest(Class~., ntree = B, data = train)
phatimb <- predict(modelimb, test, type = "prob")
#AUCb
pred_rocr1 <- prediction(phatb[,2], test$Class)
auc_ROCR1 <- performance(pred_rocr1, measure = "auc")
AUCb[i] <- auc_ROCR1@y.values[[1]]
#AUCimb
pred_rocr1 <- prediction(phatimb[,2], test$Class)
auc_ROCR1 <- performance(pred_rocr1, measure = "auc")
AUCimb[i] <- auc_ROCR1@y.values[[1]]
}
model <- c("Balanced", "Imbalanced")
AUCs <- c(mean(AUCb), mean(AUCimb))
sd <- c(sqrt(var(AUCb)), sqrt(var(AUCimb)))
data.frame(model, AUCs, sd)
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
gc()
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages('rmarkdown')
