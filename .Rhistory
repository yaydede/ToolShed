auc_ROCR1 <- performance(pred_rocr1, measure = "auc")
AUCimb[i] <- auc_ROCR1@y.values[[1]]
}
model <- c("Balanced", "Imbalanced")
AUCs <- c(mean(AUCb), mean(AUCimb))
sd <- c(sqrt(var(AUCb)), sqrt(var(AUCimb)))
data.frame(model, AUCs, sd)
rm(list = ls())
load("creditcard10.RData")
creditcard10 <- df
rm(list = ls())
load("creditcard10.RData")
creditcard10 <- df
is.atomic(train[, -31])
rm(list = ls())
load("creditcard10.RData")
df$Class <- as.factor(df$Class)
ind <- sample(nrow(df), nrow(df), replace = TRUE)
ind <- unique(ind) # Otherwise it oversamples 0's
train <- df[ind, ]
test <- df[-ind, ]
is.atomic(train[, -31])
is.atomic(outdf$data)
outdf <- SMOTE(X = train[, -31], target = train$Class, K = 10, dup_size = 50)
is.atomic(outdf$data)
is.atomic(trainn$class)
trainn <- outdf$data
trainn$class <- as.factor(trainn$class) #SMOTE makes factor to "chr"!
is.atomic(trainn$class)
remotes::install_github('rstudio/bookdown')
rm(list = ls())
load("creditcard10.RData")
df$Class <- as.factor(df$Class)
AUCb <- c()
AUCimb <- c()
n = 10 # Could be 50, since the data is large for RF
B = 100
for (i in 1:n) {
set.seed(i)
ind <- sample(nrow(df), nrow(df), replace = TRUE)
ind <- unique(ind) # Otherwise it oversamples 0's
train <- df[ind, ]
test <- df[-ind, ]
# Balancing
outdf <- SMOTE(X = train[, -31], target = train$Class, K = 10, dup_size = 50)
trainn <- outdf$data
trainn$class <- as.factor(trainn$class) #SMOTE makes factor to "chr"!
colnames(trainn)[31] <- "Class" #SMOTE made it lower case!
modelb <- randomForest(Class~., ntree = B, data = trainn)
phatb <- predict(modelb, test, type = "prob")
# Without Balancing
modelimb <- randomForest(Class~., ntree = B, data = train)
phatimb <- predict(modelimb, test, type = "prob")
#AUCb
pred_rocr1 <- prediction(phatb[,2], test$Class)
auc_ROCR1 <- performance(pred_rocr1, measure = "auc")
AUCb[i] <- auc_ROCR1@y.values[[1]]
#AUCimb
pred_rocr1 <- prediction(phatimb[,2], test$Class)
auc_ROCR1 <- performance(pred_rocr1, measure = "auc")
AUCimb[i] <- auc_ROCR1@y.values[[1]]
}
library(tidyverse)
library(ROCR)
library(smotefamily)
library(randomForest)
df$Class <- as.factor(df$Class)
AUCb <- c()
AUCimb <- c()
n = 10 # Could be 50, since the data is large for RF
B = 100
for (i in 1:n) {
set.seed(i)
ind <- sample(nrow(df), nrow(df), replace = TRUE)
ind <- unique(ind) # Otherwise it oversamples 0's
train <- df[ind, ]
test <- df[-ind, ]
# Balancing
outdf <- SMOTE(X = train[, -31], target = train$Class, K = 10, dup_size = 50)
trainn <- outdf$data
trainn$class <- as.factor(trainn$class) #SMOTE makes factor to "chr"!
colnames(trainn)[31] <- "Class" #SMOTE made it lower case!
modelb <- randomForest(Class~., ntree = B, data = trainn)
phatb <- predict(modelb, test, type = "prob")
# Without Balancing
modelimb <- randomForest(Class~., ntree = B, data = train)
phatimb <- predict(modelimb, test, type = "prob")
#AUCb
pred_rocr1 <- prediction(phatb[,2], test$Class)
auc_ROCR1 <- performance(pred_rocr1, measure = "auc")
AUCb[i] <- auc_ROCR1@y.values[[1]]
#AUCimb
pred_rocr1 <- prediction(phatimb[,2], test$Class)
auc_ROCR1 <- performance(pred_rocr1, measure = "auc")
AUCimb[i] <- auc_ROCR1@y.values[[1]]
}
model <- c("Balanced", "Imbalanced")
AUCs <- c(mean(AUCb), mean(AUCimb))
sd <- c(sqrt(var(AUCb)), sqrt(var(AUCimb)))
data.frame(model, AUCs, sd)
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
gc()
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages('rmarkdown')
knitr::include_graphics("png/rnn.png")
library(tsibble)
library(fpp3)
load("~/Dropbox/ToolShed_draft/toronto2.rds")
toronto2 <- data
df <- toronto2 %>%
mutate(dcases = difference(cases),
dmob = difference(mob),
ddelay = difference(delay),
dmale = difference(male),
dtemp = difference(temp),
dhum = difference(hum))
dft <- df[ ,-c(2:5,7,8)] #removing levels
dft <- dft[-1, c(3:7,2)] # reordering the columns
sdtf <- scale(dft) #
head(sdtf)
tensorin <- function(l, x){
maxl = l+1
xm <- embed(x, maxl)
xm <- xm[, -c(2:3)]
n <- nrow(xm)
f1 <- data.matrix(xm[, -1])
y <- xm[, 1]
f2 <- array(f1, c(n, ncol(x), l))
f3 <- f2[,, l:1]
f4 <- aperm(f3, c(1, 3, 2))
list(f4, y)
}
tensored <- tensorin(5, toydata)
trnt <- tensorin(7, sdtf)
X <- trnt[1]
y <- trnt[2]
X[[1]][1,,]
y[[1]][1]
library(keras)
model <- keras_model_sequential() %>%
layer_simple_rnn(units = 12,
input_shape = list(7, 6),
dropout = 0.1, recurrent_dropout = 0.1) %>%
layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(),
loss = "mse")
dim(X[[1]])
train <- 1:208
test <- 208:dim(X[[1]])[1]
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 28, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test])
)
y_act <- y[[1]][test]
var_y <- var(y_act)
yhat <- predict(model, X[[1]][test,, ])
1 - mean((yhat -y_act)^2) / var_y # R^2
sqrt(mean((yhat -y_act)^2)) # RMSPE
plot(y[[1]][test], type ="l", col = "blue",
ylab = "Actual (Blue) vs. Prediction (Red)",
xlab = "Last 50 Days",
main = "RNN Forecasting for Covid-19 Cases")
lines(kpred, col = "red")
plot(y[[1]][test], type ="l", col = "blue",
ylab = "Actual (Blue) vs. Prediction (Red)",
xlab = "Last 50 Days",
main = "RNN Forecasting for Covid-19 Cases")
lines(yhat, col = "red")
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 28, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
verbose = 0
)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 28, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
verbose = 0
)
plot(history)
plot(y[[1]][test], type ="l", col = "blue",
ylab = "Actual (Blue) vs. Prediction (Red)",
xlab = "Last 50 Days",
main = "RNN Forecasting for Covid-19 Cases")
lines(yhat, col = "red", lwd = 2)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 28, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
verbose = 0
)
plot(history)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 28, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
#verbose = 0
)
plot(history)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 28, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
#verbose = 0
)
plot(history)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 8, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
verbose = 0
)
plot(history)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 38, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
#verbose = 0
)
plot(history)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 24, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
#verbose = 0
)
plot(history)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
#verbose = 0
)
plot(history)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 1, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
#verbose = 0
)
plot(history)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 208, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
#verbose = 0
)
plot(history)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
#verbose = 0
)
plot(history)
y_act <- y[[1]][test]
var_y <- var(y_act)
yhat <- predict(model, X[[1]][test,, ])
1 - mean((yhat -y_act)^2) / var_y # R^2
sqrt(mean((yhat -y_act)^2)) # RMSPE
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 36, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
#verbose = 0
)
plot(history)
y_act <- y[[1]][test]
var_y <- var(y_act)
yhat <- predict(model, X[[1]][test,, ])
1 - mean((yhat -y_act)^2) / var_y # R^2
sqrt(mean((yhat -y_act)^2)) # RMSPE
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 1, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
#verbose = 0
)
plot(history)
y_act <- y[[1]][test]
var_y <- var(y_act)
yhat <- predict(model, X[[1]][test,, ])
1 - mean((yhat -y_act)^2) / var_y # R^2
sqrt(mean((yhat -y_act)^2)) # RMSPE
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 208, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
#verbose = 0
)
plot(history)
y_act <- y[[1]][test]
var_y <- var(y_act)
yhat <- predict(model, X[[1]][test,, ])
1 - mean((yhat -y_act)^2) / var_y # R^2
sqrt(mean((yhat -y_act)^2)) # RMSPE
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 100,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
#verbose = 0
)
plot(history)
library(keras)
model <- keras_model_sequential() %>%
layer_simple_rnn(units = 24,
input_shape = list(7, 6),
dropout = 0.1, recurrent_dropout = 0.1) %>%
layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(),
loss = "mse")
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
verbose = 0
)
plot(history)
y_act <- y[[1]][test]
var_y <- var(y_act)
yhat <- predict(model, X[[1]][test,, ])
1 - mean((yhat -y_act)^2) / var_y # R^2
sqrt(mean((yhat -y_act)^2)) # RMSPE
plot(y[[1]][test], type ="l", col = "blue",
ylab = "Actual (Blue) vs. Prediction (Red)",
xlab = "Last 50 Days",
main = "RNN Forecasting for Covid-19 Cases")
lines(yhat, col = "red", lwd = 2)
library(keras)
model <- keras_model_sequential() %>%
layer_simple_rnn(units = 36,
input_shape = list(7, 6),
dropout = 0.1, recurrent_dropout = 0.1) %>%
layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(),
loss = "mse")
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
verbose = 0
)
plot(history)
y_act <- y[[1]][test]
var_y <- var(y_act)
yhat <- predict(model, X[[1]][test,, ])
1 - mean((yhat -y_act)^2) / var_y # R^2
sqrt(mean((yhat -y_act)^2)) # RMSPE
library(keras)
model <- keras_model_sequential() %>%
layer_simple_rnn(units = 24,
input_shape = list(7, 6),
dropout = 0.1, recurrent_dropout = 0.1) %>%
layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(),
loss = "mse")
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
verbose = 0
)
plot(history)
y_act <- y[[1]][test]
var_y <- var(y_act)
yhat <- predict(model, X[[1]][test,, ])
1 - mean((yhat -y_act)^2) / var_y # R^2
sqrt(mean((yhat -y_act)^2)) # RMSPE
plot(y[[1]][test], type ="l", col = "blue",
ylab = "Actual (Blue) vs. Prediction (Red)",
xlab = "Last 50 Days",
main = "RNN Forecasting for Covid-19 Cases")
lines(yhat, col = "red", lwd = 2)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 150,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
verbose = 0
)
plot(history)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
verbose = 0
)
plot(history)
library(keras)
model <- keras_model_sequential() %>%
layer_simple_rnn(units = 24,
input_shape = list(7, 6),
dropout = 0.1, recurrent_dropout = 0.1) %>%
layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(),
loss = "mse")
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
verbose = 0
)
plot(history)
y_act <- y[[1]][test]
var_y <- var(y_act)
yhat <- predict(model, X[[1]][test,, ])
1 - mean((yhat -y_act)^2) / var_y # R^2
sqrt(mean((yhat -y_act)^2)) # RMSPE
y_act <- y[[1]][test]
var_y <- var(y_act)
yhat <- predict(model, X[[1]][test,, ])
1 - mean((yhat -y_act)^2) / var_y # R^2
sqrt(mean((yhat -y_act)^2)) # RMSPE
y_act <- y[[1]][test]
var_y <- var(y_act)
yhat <- predict(model, X[[1]][test,, ])
1 - mean((yhat -y_act)^2) / var_y # R^2
sqrt(mean((yhat -y_act)^2)) # RMSPE
plot(y[[1]][test], type ="l", col = "blue",
ylab = "Actual (Blue) vs. Prediction (Red)",
xlab = "Last 50 Days",
main = "RNN Forecasting for Covid-19 Cases")
lines(yhat, col = "red", lwd = 2)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 175,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
verbose = 0
)
plot(history)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 175,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
verbose = 0
)
plot(history)
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
verbose = 0
)
plot(history)
library(keras)
model <- keras_model_sequential() %>%
layer_simple_rnn(units = 24,
input_shape = list(7, 6),
dropout = 0.1, recurrent_dropout = 0.1) %>%
layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(),
loss = "mse")
history <- model %>% fit(
X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 75,
validation_data =
list(X[[1]][test,, ], y[[1]][test]),
verbose = 0
)
plot(history)
y_act <- y[[1]][test]
var_y <- var(y_act)
yhat <- predict(model, X[[1]][test,, ])
1 - mean((yhat -y_act)^2) / var_y # R^2
sqrt(mean((yhat -y_act)^2)) # RMSPE
plot(y[[1]][test], type ="l", col = "blue",
ylab = "Actual (Blue) vs. Prediction (Red)",
xlab = "Last 50 Days",
main = "RNN Forecasting for Covid-19 Cases")
lines(yhat, col = "red", lwd = 2)
# array
x1 = c(1, 2, 3)
x2 = c(4, 5, 6, 7, 8, 9)
adata <- array(c(x1, x2), dim = c(3,3,2))
dim(adata)
adata
adata[1,,]
# Data
toydata <- matrix(c(1:100, 101:200, 201:300), 100)
colnames(toydata) <- c("y", "x1", "x2")
head(toydata)
datam <- embed(toydata, 6)
datam <- datam[, -c(2:3)]
n <- nrow(datam)
f1 <- data.matrix(datam[, -1])
f2 <- array(f1, c(n, 3, 5))
f3 <- f2[,, 5:1]
f4 <- aperm(f3, c(1, 3, 2))
f4[1,,]
f4[2,,]
# Data
toydata <- matrix(c(1:100, 101:200, 201:300), 100)
colnames(toydata) <- c("y", "x1", "x2")
head(toydata)
datam <- embed(toydata, 6)
datam <- datam[, -c(2:3)]
datam <- embed(toydata, 6)
datam <- datam[, -c(2:3)]
head(datam)
n <- nrow(datam)
f1 <- data.matrix(datam[, -1]) # Removing Y
f2 <- array(f1, c(n, 3, 5))
f2[1,,]
f3 <- f2[,, 5:1]
f3[1,,]
f4 <- aperm(f3, c(1, 3, 2))
f4[1,,]
